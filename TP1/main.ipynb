{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6250c6c-ab1c-40f0-b6d0-fc437bedbb99",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## What about the LLMs?\n",
    "\n",
    "**You must write the answer to this question in a notebook hosted in your github account and give access to your supervisor.**\n",
    "\n",
    "LLMs are reputed to have revolutionised automatic language processing. Since the introduction of BERT-type models, all language processing applications have been based on LLMs, of varying degrees of sophistication and size. These models are trained on multiple tasks and are therefore capable of performing new tasks without learning, simply from a prompt. This is known as \"zero-shot learning\" because there is no learning phase as such. We are going to test these models on our classification task.\n",
    "\n",
    "Huggingface is a Franco-American company that develops tools for building applications based on Deep Learning. In particular, it hosts the huggingface.co portal, which contains numerous Deep Learning models. These models can be used very easily thanks to the [Transformer] library (https://huggingface.co/docs/transformers/quicktour) developed by HuggingFace.\n",
    "\n",
    "Using a transform model in zero-shot learning with HuggingFace is very simple: [see documentation](https://huggingface.co/tasks/zero-shot-classification)\n",
    "\n",
    "However, you need to choose a suitable model from the list of models compatible with Zero-Shot classification. HuggingFace offers [numerous models](https://huggingface.co/models?pipeline_tag=zero-shot-classification). \n",
    "\n",
    "The classes proposed to the model must also provide sufficient semantic information for the model to understand them.\n",
    "\n",
    "**Question**:\n",
    "\n",
    "* Write a code to classify an example of text from an article in Le Monde using a model transformed using zero-sot learning with the HuggingFace library.\n",
    "* choose a model and explain your choice\n",
    "* choose a formulation for the classes to be predicted\n",
    "* show that the model predicts a class for the text of the article (correct or incorrect, analyse the results)\n",
    "* evaluate the performance of your model on 100 articles (a test set).\n",
    "* note model sizes, processing times and classification results\n",
    "\n",
    "\n",
    "Notes :\n",
    "* make sure that you use the correct Tokenizer when using a model \n",
    "* start testing with a small number of articles and the first 100's of characters for faster experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d5bf46-7398-499b-aaf3-c01298913edb",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "***Model***: we would like a model whose base architecture is performant, which was trained on a corpus including a large number of French newspaper article, and which can process up to 4000 tokens, i.e., the approximate max lenght of our documents. As far as I know, all models available for zero-shot classification on HuggingFace match the first two conditions, and none match the last, meaning the documents longuer than 512 tokens will be truncated (architectures like Longformer and BigBird can process longer sequences but are not optimazed for zero-shot classification). After experimenting with several models, I obtained the best results with [MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli](https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli). The reason is probably that it is one of the biggest model available, with 435M parameters. This makes it slower, but not exceedingly with GPU acceleration (it runs 5.5x faster than on CPU on a M2 Max with 38 GPU cores).\n",
    "\n",
    "***Formulation of the classes***: I created a dictionary to replace the three-letter section codes with their full names before feeding them into the classifier. This provides the model with more meaningful information. This step proved necessary to achieve correct performance, as the abbreviated codes alone do not provide enough context.\n",
    "\n",
    "***Tokenization:*** it is performed automatically by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b316c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('https://cloud.teklia.com/index.php/s/isNwnwA7a7AWst6/download/LeMonde2003_9classes.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b77a80-93ec-489a-8154-ac4624321e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1046 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest document includes 3817 tokens.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "max_token_length = data['text'].astype(str).apply(lambda x: len(tokenizer.tokenize(x))).max()\n",
    "print(f'The longest document includes {max_token_length} tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25502e43-5859-492c-abe7-4e86d0ff7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available(): device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
    "else: device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a53c6-be25-48f1-bc2c-22b0b2c10e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create the zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                     model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\",\n",
    "                     framework=\"pt\", # Using PyTorch to avoid a conflict with Keras\n",
    "                     device=device)\n",
    "\n",
    "# Associate the sections' code with their full name\n",
    "label_codes = {\n",
    "    'sports': 'SPO',\n",
    "    'arts': 'ART',\n",
    "    'France': 'FRA',\n",
    "    'société': 'SOC',\n",
    "    'international': 'INT',\n",
    "    'entreprises': 'ENT',\n",
    "    'une': 'UNE'\n",
    "}\n",
    "\n",
    "# Pass the full name to the classifier, than revert its output back to code\n",
    "def predict_category(text, labels):\n",
    "    result = classifier(text, candidate_labels=list(label_codes.keys()))\n",
    "    predicted_label = result['labels'][0]\n",
    "    return label_codes[predicted_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931bc69-e8f5-447b-8c77-8cf19d0ee680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single sample\n",
    "random_idx = 423\n",
    "sample_text = data['text'].iloc[random_idx]\n",
    "sample_label = data['category'].iloc[random_idx]\n",
    "\n",
    "# Get detailed predictions\n",
    "results = classifier(sample_text, candidate_labels=list(label_codes.keys()))\n",
    "\n",
    "# Print all predictions in descending order\n",
    "print(f\"\\nPredicted categories for sample #{random_idx}:\")\n",
    "predictions = sorted(zip(results['labels'], results['scores']), \n",
    "                    key=lambda x: x[1], \n",
    "                    reverse=True)\n",
    "\n",
    "for label, score in predictions:\n",
    "    print(f\"{label}: {score:.3f}\")\n",
    "\n",
    "# Check if the prediction is correct\n",
    "top_prediction = label_codes[predictions[0][0]]\n",
    "\n",
    "is_correct = top_prediction == sample_label\n",
    "print(f\"\\nTop prediction ({top_prediction}) {'matches' if is_correct else 'does not match'} true label ({sample_label}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee91360-af86-4e35-99a9-296c13681acd",
   "metadata": {},
   "source": [
    "***Comment:*** The classifier accurately predicts the class of document #423, with a very high level of certainty. I assume the reason is that an article on international relations would include the word international and its derivatives many times, but not the other categories names, except maybe for France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8ea33-c124-435f-9b57-26ed1eed35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Select 100 samples (with fixed seed for reproductibility)\n",
    "sample_data = data.sample(n=100, random_state=86)\n",
    "\n",
    "# Predict for each sample\n",
    "tqdm.pandas()\n",
    "sample_data['predicted_category'] = sample_data['text'].progress_apply(\n",
    "    lambda x: predict_category(x, list(label_codes.keys()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd5780-47b8-445c-836c-2ddf08528769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assess performance\n",
    "y_true = sample_data['category']\n",
    "y_pred = sample_data['predicted_category']\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=list(label_codes.keys()), yticklabels=list(label_codes.keys()))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764450f-540d-4eee-8ece-18aeffe7b6a5",
   "metadata": {},
   "source": [
    "***Comment:*** since the model is pretrained, there’s no need to split the data into training and testing sets. Performance can be assessed directly on a random sample of documents. The results show a significant performance contrast: the model performs very well on four categories—ART, ENT, INT, and SPO—but poorly on the remaining three—FRA, SOC, and UNE. The reason is likely the same as above: as titles, Arts, Enterprises, International and Sports provide highly informative content regarding their sections, whereas France, Society and Une are more ambiguous. Indeed, any topic can appear in the UNE category, while SOC and FRA tend to encompass a wide range of subjects that don’t belong to more specialized categories. The classifier is likely to struggle with these categories unless it is provided with more detailed information about their content.\n",
    "\n",
    "Summary:\n",
    "- Model size: 435M parameters\n",
    "- Processing time: 1:01 for 100 documents, i.e., ≈ 0.6 secund per document in average\n",
    "- Classification results: weighted average of precision, recall and f1 score are 0.52, 0.45 and 0.45 respectively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
