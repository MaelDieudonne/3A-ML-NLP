{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08144275-905a-4829-836f-517d8294e17f",
   "metadata": {},
   "source": [
    "# Fine-tuning RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68687f9f-dfc5-45ba-9bbc-9758e4b6e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2ecc8-6384-471b-964d-f1a536724bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
    "else: device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551f14c-1c9a-422b-97fe-a915300b0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"output/models/\", exist_ok=True)\n",
    "os.makedirs(\"output/preds/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39025866-c7e2-42e0-a78e-026e977f62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/processed/train.csv\")\n",
    "test = pd.read_csv(\"data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e5802-7413-4c26-bdb5-aa8209117af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.sample(frac=0.005).reset_index(drop=True)\n",
    "# test = test.sample(frac=0.005).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4666db-a10f-4c4f-9375-5af042efa054",
   "metadata": {},
   "source": [
    "# 1. Build the model\n",
    "\n",
    "Firstly, let's load the **base model** we intend to fine-tune. RoBERTa is initialized with an untrained classifier head consisting of two dense (= linear) layers:\n",
    "- The first projects the output from the base model to a hidden space of the same dimensionality (1024 for RoBERTa-large).\n",
    "- The second maps the hidden representation to the label space, reducing the dimensionality accordingly (to 2 for a binary classifier).\n",
    "\n",
    "This classifier head is not pretrained, hence the warning below. It can either be fine-tuned directly or replaced with a custom classifier block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e44976-2496-4b45-8d49-fe333395a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed7cf0-c67a-40bc-a087-900aed7f93f8",
   "metadata": {},
   "source": [
    "Fine-tuning is usually restricted to the final layers of the model responsible for the desired task. This approach speeds up training while preserving the knowledge acquired by the model during pretraining. Since we plan to implement a custom classifier block, I simply freeze all parameters of the base model (they are still trainable by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111248d-8d7f-4613-a550-62d9c33a9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained layers \n",
    "print(f\"Number of parameters in base RoBERTa: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M\")\n",
    "\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Number of trainable parameters after freezing: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fd112-8154-482a-b735-3d11905a6483",
   "metadata": {},
   "source": [
    "There are two key decisions for the classifier head: which layers to include, and which input to use.\n",
    "\n",
    "Regarding the **layers**, a typical approach is to insert an activation layer and a dropout layer between the two projection layers of the initial classification head. The activation layer introduces non-linearity, while the dropout layer helps prevent overfitting. I also add a normalization layer and another dropout layer after the activation layer, as well as an intermediate dense layer to avoid going at once from dimension 1024 to 2. A simpler classifier head can be obtained by retaining only the last 2 dense layers. A final Softmax layer is necessary to obtain probabilities, and it is typically placed in the `predict` function (instead of `forward`) to simplify the computation of the loss.\n",
    "\n",
    "Regarding the **input**:\n",
    "- The simplest approach is to focus on the `[CLS]` token, which marks the beginning of the input sequence and serves as an embedding for its entirety: `x = outputs.last_hidden_state[:, 0, :]`\n",
    "- Another approach is to pool all tokens from the input sequence, either by taking their mean or maximum in each dimension: `x = torch.mean(outputs.last_hidden_state, dim=1)` or  `x = torch.max(outputs.last_hidden_state, dim=1).values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a0a3fa-97d2-4b85-9a65-e657bde14507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layers for classification to finetune\n",
    "class RoBERTa_architecture(nn.Module): \n",
    "    def __init__(self, roberta_model): \n",
    "        super(RoBERTa_architecture, self).__init__() \n",
    "        self.roberta = roberta_model\n",
    "        # Dense layer 1 => input layer\n",
    "        self.in_proj = nn.Linear(1024, 1024)  # Disable for simplex model\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)  # Disable for simplex model\n",
    "        # GELU\n",
    "        self.gelu = nn.GELU()  # Disable for simplex model\n",
    "        # Layer nom\n",
    "        self.layer_norm = nn.LayerNorm(1024, eps=1e-5)  # Disable for simplex model\n",
    "        # Dense layer 2 => intermediate layer\n",
    "        self.inter_proj = nn.Linear(1024, 512)\n",
    "        # Dense layer 3 => output layer\n",
    "        self.out_proj = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, sent_id, mask): \n",
    "        # Pass inputs to the base model\n",
    "        outputs = self.roberta(sent_id, attention_mask=mask, return_dict=True)\n",
    "        # Get CLS token\n",
    "        x = outputs.last_hidden_state[:, 0, :]\n",
    "        # Apply new layers\n",
    "        x = self.in_proj(x)  # Disable for simplex model\n",
    "        x = self.dropout(x)  # Disable for simplex model\n",
    "        x = self.gelu(x)  # Disable for simplex model\n",
    "        x = self.layer_norm(x)  # Disable for simplex model\n",
    "        x = self.inter_proj(x)\n",
    "        x = self.dropout(x)  # Disable for simplex model\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, sent_id, mask):\n",
    "        self.eval()\n",
    "        # Disable gradient calculation for inference\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(sent_id, mask)\n",
    "        # Apply Softmax\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # Retain highest probability\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e579d-d7d9-4b07-8cd5-5b3057e98255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "roberta_classifier = RoBERTa_architecture(model.roberta)\n",
    "roberta_classifier = roberta_classifier.to(device)\n",
    "\n",
    "# Map the labels\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "roberta_classifier.id2label = id2label\n",
    "roberta_classifier.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6660cada-4193-4c28-8165-f3989846593a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chech which layers are available for training\n",
    "for name, param in roberta_classifier.named_parameters(): \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7a9c2d8-46e6-449f-bb00-61c70618d5e4",
   "metadata": {},
   "source": [
    "As expected, only 4 of the new layers are trainable:\n",
    "- The first dense layer, making for $1024*1024+1024=1,049,600$ trainable parameters\n",
    "- The normalization layer, making for $1024*2=2,048$ trainable parameters\n",
    "- The secund dense layer, making for $1024*512+512=524,800$ trainable parameters\n",
    "- The final dense layer, making for $512*2+2=1,026$ trainable parameters\n",
    "\n",
    "The total number of trainable parameters should thus be $1,577,474$ (or $525,826$ for the simpler model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81282f3e-9536-4406-afb1-a148d83f113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters to fine-tune: {sum(p.numel() for p in roberta_classifier.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00ce8d-a2b1-4862-ac89-fb8a5d632ca0",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "First, we need a tokenizer. RoBERTa uses Byte-Pair Encoding (BPE) which operates at the subword level with an agglomerative approach. It merges the most frequent pairs iteratively until the desired vocabulary size is reached, which is 50,265 for RoBERTa. \n",
    "\n",
    "RoBERTa has a maximum context length of 512 tokens. Sequences longer than this must be truncated before being passed to the model. We opt for left-side truncation to preserve summaries that sometimes appear at the end of reviews. An alternative approach would be to split long sequences into smaller chunks, process them sequentially, and average the results. However, this would be more complex to implement and may not yield reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791922b-f2fc-497f-ba8b-fcab31ecca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-large', truncation_side = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9bec0b-6128-4455-a95e-0cbdf06ab48d",
   "metadata": {},
   "source": [
    "RoBERTa requires all input sequences to be of the same length. Longer sequences are truncated, as discussed above, while shorter sequences are padded by adding empty tokens like `[PAD]` at the end. A mask is also generated to help the model identify the padded tokens. Padding can be applied either to the maximum context length of the model or to the longest sequence in the batch, which is the option used here. RoBERTa does not use `token_type_ids`, which are typically employed to distinguish between different parts of the input, such as prompts and answers. As a result, they are disabled here. Lastly, we pass the `review_id` variable to the `dataloader` so that we can match the predictions with the original dataset once the model is trained.\n",
    "\n",
    "`batch_size` defines how many sequences are processed in parallel during a single batch. Larger batch sizes can improve computational efficiency, provided there is enough available memory. They offer more stable gradient estimates by using larger amounts of data, but they result in less frequent updates to the model. As a consequence, larger batch sizes may require more epochs to converge, as well as a lower learning rate to reduce the risk of overfitting. Here, `batch_size` is set to 250, meaning that 1% of the dataset is processed in each batch. This setup consumes up to 25 Go of memory. There is no downside to using a higher `batch_size` for inference, so we set it to 400 to fully utilize the 64 GB of available memory on our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a67cc3-6ffc-4c1e-8def-5c01d3b654b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, tokenizer, label2id):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_batch_size = 250\n",
    "        self.test_batch_size = 400\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def prepare(self, data, is_train=True):\n",
    "        tokens = self.tokenizer(\n",
    "            data['text'].tolist(),\n",
    "            padding = 'longest',\n",
    "            truncation = True,\n",
    "            return_token_type_ids = False,\n",
    "            return_tensors = 'pt')\n",
    "        # Tokenize and convert to tensor\n",
    "        input_ids = tokens['input_ids'].clone().detach()\n",
    "        attention_mask = tokens['attention_mask'].clone().detach()\n",
    "        # Get sequence lengths\n",
    "        seq_lengths = [len(seq) for seq in input_ids]\n",
    "        # Convert labels\n",
    "        numeric_labels = [self.label2id[label] for label in data['sentiment']]\n",
    "        labels = torch.tensor(numeric_labels, dtype = torch.long)\n",
    "        # Extract review IDs\n",
    "        ids = torch.tensor(data['review_id'].tolist(), dtype = torch.long)\n",
    "        # Choose the appropriate batch size\n",
    "        batch_size = self.train_batch_size if is_train else self.test_batch_size\n",
    "        # Create dataset and dataloader\n",
    "        dataset = TensorDataset(input_ids, attention_mask, labels, ids)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n",
    "\n",
    "        return dataloader, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085aab8-4c3b-4938-b736-52bf554f7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "\n",
    "preprocessor = DataPreprocessor(tokenizer, label2id)\n",
    "train_dataloader, train_seq_lengths = preprocessor.prepare(train, is_train=True)\n",
    "test_dataloader, test_seq_lengths = preprocessor.prepare(test, is_train=False)\n",
    "\n",
    "time1 = time.time()\n",
    "print(f\"Tokenization duration: {time1-time0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4339f-19b7-41a7-b722-82b33512101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average length of sequences in train: {sum(train_seq_lengths) / len(train_seq_lengths):.0f}\")\n",
    "print(f\"Average length of sequences in test: {sum(test_seq_lengths) / len(test_seq_lengths):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edfe8ea-2e06-4a1e-8ddd-3ffdd144f1fc",
   "metadata": {},
   "source": [
    "After tokenization and dataloading, the average length of sequences in 512. This indicates that there is at least one sequences longer than 512 tokens in each batch, so that `padding = 'longest'` has the same effect as `padding = 'max_length'`. With a `batch_size` of 250, there are exactly $250Ã—512=128,000$ tokens per (training) batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4f1ac-246b-4859-8755-03dbf37834eb",
   "metadata": {},
   "source": [
    "# 3. Training\n",
    "\n",
    "We define functions for **training** and **testing**. Since we plan to evaluate the model on the entire test dataset after each epoch, we store the logits returned at that time. This allows us to retrieve predictions directly, avoiding the need to recompute them. Additionally, we save the true labels and review IDs to link the predictions with the original dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81efe6f-e9e9-4069-b33a-40ea8e687d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67fdab-6373-49aa-8ae6-abacf58788f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train_losses = []\n",
    "batch_train_accuracy = []\n",
    "\n",
    "def model_train(batch):\n",
    "    b_input_ids, b_attention_mask, b_labels, _ = [t.to(device) for t in batch]\n",
    "    # Reset gradients before backpropagation\n",
    "    roberta_classifier.zero_grad()\n",
    "    # Perform a forward pass to calculate outputs\n",
    "    logits = roberta_classifier(b_input_ids, b_attention_mask)\n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(logits, b_labels)\n",
    "    batch_train_losses.append(loss.item())\n",
    "    # Calculate accuracy\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    batch_train_accuracy.append((preds == b_labels).sum().item() / b_labels.size(0))\n",
    "    # Backpropagate the loss\n",
    "    loss.backward()\n",
    "    # Update model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfce21a-d6cb-4854-b7db-5cd307713288",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test_losses = []\n",
    "batch_test_accuracy = []\n",
    "\n",
    "def model_eval(batch):\n",
    "    b_input_ids, b_attention_mask, b_labels, b_ids = [t.to(device) for t in batch]\n",
    "    # Forward pass\n",
    "    logits = roberta_classifier(b_input_ids, b_attention_mask)\n",
    "    # Store results \n",
    "    all_logits.append(logits.detach().cpu())\n",
    "    all_labels.append(b_labels.detach().cpu())\n",
    "    all_ids.append(b_ids.detach().cpu())\n",
    "    # Calculate loss\n",
    "    loss = loss_fn(logits, b_labels)\n",
    "    batch_test_losses.append(loss.item())\n",
    "    # Calculate accuracy\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    batch_test_accuracy.append((preds == b_labels).sum().item() / b_labels.size(0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae023e-45a5-4d3c-9a7d-b866d91c9fc4",
   "metadata": {},
   "source": [
    "We define a **training loop** with the ability to resume after interruption.\n",
    "\n",
    "- The training dataset is processed `num_epochs` times. Typical values for fine-tuning range between 3 to 12. We set it to 12 and will compute the test error after every batch to detect when the model starts overfitting.\n",
    "\n",
    "- For the **optimizer**, we set a relatively low learning rate, as is commonly recommended for fine-tuning RoBERTa models. The `weight_decay` parameter is a regularization term that adds a penalty for large weights during optimization, helping to mitigate overfitting.\n",
    "\n",
    "- We use a **scheduler** to further reduce the learning rate as the model converges. The `num_warmup_steps` is another regularization parameter, which gradually reduces the learning rate during the first batches of each epoch to prevent instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ce885-1fc1-4e91-acbe-15ff237d8fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 22\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "print(f\"Number of training steps: {num_training_steps}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    roberta_classifier.parameters(),\n",
    "    lr = 1e-4,\n",
    "    weight_decay = 0.03,\n",
    "    eps = 1e-6)\n",
    "\n",
    "# Scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = 0.1 * num_training_steps,\n",
    "    num_training_steps = num_training_steps)\n",
    "\n",
    "# Avoid issues with multithreading\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da1f41-30ea-4e1b-9706-74390dde444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training\n",
    "def get_latest_checkpoint(model_dir):\n",
    "    # Verify if checkpoints are present\n",
    "    checkpoint_files = glob.glob(os.path.join(model_dir, \"epoch_*.pth\"))\n",
    "    if not checkpoint_files:\n",
    "        return None, 0\n",
    "    # Extract epoch numbers from filenames\n",
    "    epochs = [int(os.path.basename(f).split(\"_\")[1].split(\".\")[0]) for f in checkpoint_files]\n",
    "    last_epoch = max(epochs)\n",
    "    return os.path.join(model_dir, f\"epoch_{last_epoch}.pth\"), last_epoch\n",
    "\n",
    "latest_checkpoint, last_epoch = get_latest_checkpoint(\"output/models\")\n",
    "\n",
    "if latest_checkpoint:\n",
    "    if last_epoch >= num_epochs:\n",
    "        print(f\"Training already completed up to epoch {last_epoch}.\")\n",
    "    else:\n",
    "        checkpoint = torch.load(latest_checkpoint, weights_only=False)\n",
    "        roberta_classifier.load_state_dict(checkpoint['model'], strict=False)  # Load saved parameters only\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        batch_train_losses = checkpoint['batch_train_losses']\n",
    "        batch_test_losses = checkpoint['batch_test_losses']\n",
    "        epoch_train_losses = checkpoint['epoch_train_losses']\n",
    "        epoch_test_losses = checkpoint['epoch_test_losses']\n",
    "        batch_train_accuracy = checkpoint['batch_train_accuracy']\n",
    "        batch_test_accuracy = checkpoint['batch_test_accuracy']\n",
    "        epoch_train_accuracy = checkpoint['epoch_train_accuracy']\n",
    "        accuracies = checkpoint['accuracies']\n",
    "        classif_errors = checkpoint['classif_errors']\n",
    "        batch_times_train = checkpoint['batch_times_train']\n",
    "        batch_times_test = checkpoint['batch_times_test']\n",
    "        epoch_times = checkpoint['epoch_times']\n",
    "        print(f\"Resuming from epoch {last_epoch}\")\n",
    "\n",
    "else:\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    epoch_train_accuracy = []\n",
    "    accuracies = []\n",
    "    classif_errors = []\n",
    "    batch_times_train = []\n",
    "    batch_times_test = []\n",
    "    epoch_times = []\n",
    "    print(f\"No checkpoint found, starting over...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c38e0b-625a-46c1-80f7-25230260e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory tracking\n",
    "if device == 'mps':\n",
    "    torch.mps.empty_cache()\n",
    "baseline_memory = psutil.virtual_memory().available\n",
    "max_memory_test = 0\n",
    "max_memory_train = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c76e2b-3bed-4384-ac82-a448775b6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_progress = tqdm(range(last_epoch, num_epochs),\n",
    "                      desc = \"Epochs\",\n",
    "                      position = 0,\n",
    "                      unit = \"epoch\")\n",
    "\n",
    "for epoch in epoch_progress:\n",
    "    epoch_st = time.time()\n",
    "    \n",
    "    # =============================\n",
    "    #           Training\n",
    "    # =============================\n",
    "    # Set up the train progress bar\n",
    "    train_progress = tqdm(total = len(train_dataloader), \n",
    "                          desc = f\"Train\", \n",
    "                          position = 1, \n",
    "                          leave = False,\n",
    "                          unit = \"batch\")\n",
    "    \n",
    "    roberta_classifier.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch_st = time.time()\n",
    "        loss = model_train(batch)\n",
    "        # Update progress bar with current loss\n",
    "        train_progress.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        train_progress.update(1)\n",
    "        # Track memory use and computation time\n",
    "        max_memory_train = max(max_memory_train, psutil.virtual_memory().available)\n",
    "        batch_times_train.append(time.time() - batch_st)\n",
    "    train_progress.close()\n",
    "    \n",
    "    # Calculate average train loss over the epoch\n",
    "    avg_train_loss = np.mean(batch_train_losses[-len(train_dataloader):])\n",
    "    epoch_train_losses.append(avg_train_loss)\n",
    "    # Calculate average train accuracy over the epoch\n",
    "    avg_train_accuracy = np.mean(batch_train_accuracy[-len(train_dataloader):])\n",
    "    epoch_train_accuracy.append(avg_train_accuracy)\n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    \n",
    "    # =============================\n",
    "    #            Testing\n",
    "    # =============================\n",
    "    # Set up the test progress bar\n",
    "    test_progress = tqdm(total = len(test_dataloader), \n",
    "                         desc = f\"Test\", \n",
    "                         position = 2, \n",
    "                         leave = False,\n",
    "                         unit = \"batch\")\n",
    "    \n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    all_ids = []\n",
    "    \n",
    "    roberta_classifier.eval()\n",
    "    for batch in test_dataloader:\n",
    "        batch_st = time.time()\n",
    "        loss = model_eval(batch)\n",
    "        # Update progress bar with current loss\n",
    "        test_progress.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        test_progress.update(1)\n",
    "        # Track memory use and computation time\n",
    "        max_memory_test = max(max_memory_test, psutil.virtual_memory().available)\n",
    "        batch_times_test.append(time.time() - batch_st)\n",
    "    test_progress.close()\n",
    "    \n",
    "    # Calculate average test loss over the epoch\n",
    "    avg_test_loss = np.mean(batch_test_losses[-len(test_dataloader):])\n",
    "    epoch_test_losses.append(avg_test_loss)\n",
    "    # Calculate classification error\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    probs = F.softmax(all_logits, dim=1).detach()\n",
    "    prob_class_0 = probs[:, 0]\n",
    "    prob_class_1 = probs[:, 1]\n",
    "    classif_error = (1 - torch.max(prob_class_0, prob_class_1)).mean().item()\n",
    "    classif_errors.append(classif_error)\n",
    "    # Calculate accuracy\n",
    "    preds_array = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    labels_array = torch.cat(all_labels, dim=0).cpu().numpy()\n",
    "    accuracy = accuracy_score(preds_array, labels_array)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "    # =============================\n",
    "    #             Saving\n",
    "    # =============================\n",
    "    # Update epoch progress bar with average losses\n",
    "    epoch_progress.set_postfix(train_loss=f\"{avg_train_loss:.4f}\", test_loss=f\"{avg_test_loss:.4f}\", accuracy=f\"{accuracy:.4f}\")\n",
    "    # Save predictions\n",
    "    probs_array = probs.cpu().numpy()\n",
    "    results = pd.DataFrame(probs_array, columns=[f\"prob_class_{i}\" for i in range(probs_array.shape[1])])\n",
    "    results['true_label'] = [id2label[label] for label in labels_array]\n",
    "    results['review_id'] = torch.cat(all_ids, dim=0).detach().cpu().numpy()\n",
    "    results.to_csv(f\"output/preds/epoch_{epoch+1}.csv\", index=False)\n",
    "    # Track computation time\n",
    "    epoch_times.append(time.time() - epoch_st)\n",
    "    # Save model checkpoint\n",
    "    model_save_path = os.path.join(\"output/models\", f\"epoch_{epoch+1}.pth\")\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model': {k: v for k, v in roberta_classifier.named_parameters() if v.requires_grad},\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'batch_train_losses': batch_train_losses,\n",
    "        'batch_test_losses': batch_test_losses,\n",
    "        'epoch_train_losses': epoch_train_losses,\n",
    "        'epoch_test_losses': epoch_test_losses,\n",
    "        'batch_train_accuracy': batch_train_accuracy,\n",
    "        'batch_test_accuracy': batch_test_accuracy,\n",
    "        'epoch_train_accuracy': epoch_train_accuracy,\n",
    "        'accuracies': accuracies,\n",
    "        'classif_errors': classif_errors,\n",
    "        'batch_times_train': batch_times_train,\n",
    "        'batch_times_test': batch_times_test,\n",
    "        'epoch_times': epoch_times}\n",
    "    torch.save(checkpoint, model_save_path)\n",
    "    # Clean memory\n",
    "    if device == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "epoch_progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4120880-6937-41d1-94b4-19dd3f82d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max memory used for training is {(baseline_memory - max_memory_train) / 1e9:.2f} GB\")\n",
    "print(f\"Max memory used for evaluating is {(baseline_memory - max_memory_test) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9e844-378c-4e44-8882-17bf025e7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average computation time for training batches: {np.mean(batch_times_train):.2f} seconds\")\n",
    "print(f\"Average computation time for validation batches: {np.mean(batch_times_test):.2f} seconds\")\n",
    "print(f\"Average computation time per epoch: {np.mean(epoch_times):.2f} seconds\")\n",
    "print(f\"Total computation time: {np.sum(epoch_times):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5f1cf-85fa-4dff-a636-3e5a962846a0",
   "metadata": {},
   "source": [
    "# 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d517b-bc02-442c-b211-12d15d5298f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the model with the highest accuracy\n",
    "latest_checkpoint, last_epoch = get_latest_checkpoint(\"output/models\")\n",
    "checkpoint = torch.load(latest_checkpoint, weights_only = False)     \n",
    "accuracies = checkpoint['accuracies']\n",
    "classif_errors = checkpoint['classif_errors']\n",
    "best_epoch = accuracies.index(max(accuracies)) + 1\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "print(f\"Accuracy: {accuracies[best_epoch-1]}\")\n",
    "\n",
    "# Load its predictions\n",
    "results = pd.read_csv(f\"output/preds/epoch_{best_epoch}.csv\")\n",
    "results = pd.merge(test, results, on = 'review_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f086402-ca65-40b4-9f6d-596f19aebb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for consistency\n",
    "print(f\"Do the true labels returned by the model match the original sentiments?\")\n",
    "print(f\"Yes!\" if (results['sentiment'] == results['true_label']).all() else f\"No :'(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa6d97-8e3f-4e09-8fe1-c0b71136b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted sentiments and save\n",
    "results['RoBERTa_ft'] = np.where(results['prob_class_1'] >= 0.5, 'positive', 'negative')\n",
    "results[['review_id', 'RoBERTa_ft']].to_csv(\"output/RoBERTa_ft.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2678c49-30df-43d5-af3f-41ff0a295710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
