{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08144275-905a-4829-836f-517d8294e17f",
   "metadata": {},
   "source": [
    "# Fine-tuning RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68687f9f-dfc5-45ba-9bbc-9758e4b6e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import seaborn as sns\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2ecc8-6384-471b-964d-f1a536724bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
    "else: device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551f14c-1c9a-422b-97fe-a915300b0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"output/models/\", exist_ok=True)\n",
    "os.makedirs(\"output/preds/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39025866-c7e2-42e0-a78e-026e977f62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/processed/train.csv\")\n",
    "test = pd.read_csv(\"data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e5802-7413-4c26-bdb5-aa8209117af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.sample(frac=0.005).reset_index(drop=True)\n",
    "# test = test.sample(frac=0.005).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4666db-a10f-4c4f-9375-5af042efa054",
   "metadata": {},
   "source": [
    "# 1. Build the model\n",
    "\n",
    "First, we load the **base model** we intend to fine-tune. RoBERTa is initialized with an untrained classifier head consisting of two dense (i.e., linear) layers:\n",
    "- The first projects the output from the base model to a hidden space of the same dimensionality (1024 for RoBERTa-large).\n",
    "- The second maps the hidden representation to the label space, reducing the dimensionality accordingly (to 2 for a binary classifier).\n",
    "\n",
    "Since this classifier head is not pretrained, the warning below appears. It can either be fine-tuned directly or replaced with a custom classifier block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e44976-2496-4b45-8d49-fe333395a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ed7cf0-c67a-40bc-a087-900aed7f93f8",
   "metadata": {},
   "source": [
    "Fine-tuning is usually restricted to the final layers of the model responsible for the desired task. This approach speeds up training while preserving the knowledge acquired by the model during pretraining. Since we plan to implement a custom classifier block, we simply freeze all parameters of the base model (they are still trainable by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111248d-8d7f-4613-a550-62d9c33a9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained layers \n",
    "print(f\"Number of parameters in base RoBERTa: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M\")\n",
    "\n",
    "for param in model.parameters(): \n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Number of trainable parameters after freezing: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fd112-8154-482a-b735-3d11905a6483",
   "metadata": {},
   "source": [
    "There are two key decisions for the classifier head:\n",
    "- Which layers to include\n",
    "- Which input to use\n",
    "\n",
    "Regarding the **layers**, a typical approach is to insert an activation layer and a dropout layer between the two projection layers of the initial classification head. The activation layer introduces non-linearity, while the dropout layer helps prevent overfitting. In our case, we expand the hidden space to 3072 to align with RoBERTa's intermediate size, chose a ReLU activation function, and set the dropout rate at 15 %.\n",
    "\n",
    "Alternatively, as demonstrated [here](https://github.com/gnkhata1/Finetuning-BERT-on-Movie-Reviews-Sentiment-Analysis/blob/main/BERT%2BBiLSTM-SA.py), one can use an LSTM layer, a dropout, then a dense layer. This configuration is effective for capturing long-term dependencies across the sequences, but more computationaly expensive.\n",
    "\n",
    "A final Softmax layer is necessary to obtain probabilities, and it is typically placed in the `predict` function (instead of `forward`) to simplify the computation of the loss.\n",
    "\n",
    "Regarding the **input**:\n",
    "- The simplest approach is to focus on the `[CLS]` token, which marks the beginning of the input sequence and serves as an embedding for its entirety: `x = outputs.last_hidden_state[:, 0, :]`\n",
    "- Another approach is to pool all tokens from the input sequence, either by taking their mean or maximum: `x = torch.mean(outputs.last_hidden_state, dim=1)` or  `x = torch.max(outputs.last_hidden_state, dim=1).values()`\n",
    "- The final option is to consider all tokens in the sequence, but this is only applicable when using an LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333583d-296c-4bc7-9860-5460253250db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layers for classification to finetune\n",
    "class RoBERTa_architecture(nn.Module): \n",
    "    def __init__(self, roberta_model): \n",
    "        super(RoBERTa_architecture, self).__init__() \n",
    "        self.roberta = roberta_model\n",
    "        # Dense layer 1\n",
    "        self.fc1 = nn.Linear(1024, 3072)\n",
    "        # Relu\n",
    "        self.relu = nn.ReLU()\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        # Dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(3072, 2)\n",
    " \n",
    "    def forward(self, sent_id, mask): \n",
    "        # Pass inputs to the base model\n",
    "        outputs = self.roberta(sent_id, attention_mask=mask, return_dict=True)\n",
    "        # Get CLS token representation\n",
    "        x = outputs.last_hidden_state[:, 0, :]\n",
    "        # Apply new layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def predict(self, sent_id, mask):\n",
    "        self.eval()\n",
    "        # Disable gradient calculation for inference\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(sent_id, mask)\n",
    "        # Apply Softmax\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # Retain highest probability\n",
    "        preds = torch.argmax(x, dim=1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e579d-d7d9-4b07-8cd5-5b3057e98255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "roberta_classifier = RoBERTa_architecture(model.roberta)\n",
    "roberta_classifier = roberta_classifier.to(device)\n",
    "\n",
    "# Map the labels\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "roberta_classifier.id2label = id2label\n",
    "roberta_classifier.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6660cada-4193-4c28-8165-f3989846593a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chech which layers are available for training\n",
    "for name, param in roberta_classifier.named_parameters(): \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a9c2d8-46e6-449f-bb00-61c70618d5e4",
   "metadata": {},
   "source": [
    "As expected, only the 2 dense layers from the classifier head are trainable. They represent $1024*3072+3072$ parameters for the first layer and $3072*2+2$ for the final layer, that is, $3,154,946$ trainable parameters. This is the value we get indeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81282f3e-9536-4406-afb1-a148d83f113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters to fine-tune: {sum(p.numel() for p in roberta_classifier.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00ce8d-a2b1-4862-ac89-fb8a5d632ca0",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "First, we need a tokenizer. RoBERTa uses Byte-Pair Encoding (BPE) which operates at the subword level with an agglomerative approach. It merges the most frequent pairs iteratively until the desired vocabulary size is reached, which is 50,265 for RoBERTa. \n",
    "\n",
    "RoBERTa has a maximum context length of 512 tokens. Sequences longer than this must be truncated before being passed to the model. We opt for left-side truncation to preserve summaries that sometimes appear at the end of reviews. An alternative approach would be to split long sequences into smaller chunks, process them sequentially, and average the results. However, this would be more complex to implement and may not yield reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791922b-f2fc-497f-ba8b-fcab31ecca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-large', truncation_side = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9bec0b-6128-4455-a95e-0cbdf06ab48d",
   "metadata": {},
   "source": [
    "RoBERTa requires all input sequences to be of the same length. Longer sequences are truncated, as discussed above, while shorter sequences are padded by adding empty tokens like `[PAD]` at the end. A mask is also generated to help the model identify the padded tokens. Padding can be applied either to the maximum context length of the model or to the longest sequence in the batch, which is the option used here. RoBERTa does not use `token_type_ids`, which are typically employed to distinguish between different parts of the input, such as prompts and answers. As a result, they are disabled here. Lastly, we pass the `review_id` variable to the `dataloader` so that we can match the predictions with the original dataset once the model is trained.\n",
    "\n",
    "`batch_size` defines how many sequences are processed in parallel during a single batch. Larger batch sizes can improve computational efficiency, provided there is enough available memory. They offer more stable gradient estimates by using larger amounts of data, but they result in less frequent updates to the model. As a consequence, larger batch sizes may require more epochs to converge, as well as a lower learning rate to reduce the risk of overfitting. Here, `batch_size` is set to 250, meaning that 1% of the dataset is processed in each batch. This setup consumes up to 25 Go of memory. There is no downside to using a higher `batch_size` for inference, so we set it to 500 to fully utilize the 64 GB of available memory on our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a67cc3-6ffc-4c1e-8def-5c01d3b654b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, tokenizer, label2id):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_batch_size = 250\n",
    "        self.test_batch_size = 500\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def prepare(self, data, is_train=True):\n",
    "        # Tokenize and convert to tensor\n",
    "        tokens = self.tokenizer(\n",
    "            data['text'].tolist(),\n",
    "            padding = 'longest',\n",
    "            truncation = True,\n",
    "            return_token_type_ids = False,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = tokens['input_ids'].clone().detach()\n",
    "        attention_mask = tokens['attention_mask'].clone().detach()\n",
    "\n",
    "        # Get sequence lengths\n",
    "        seq_lengths = [len(seq) for seq in input_ids]\n",
    "\n",
    "        # Convert labels\n",
    "        numeric_labels = [self.label2id[label] for label in data['sentiment']]\n",
    "        labels = torch.tensor(numeric_labels, dtype = torch.long)\n",
    "\n",
    "        # Extract review IDs\n",
    "        ids = torch.tensor(data['review_id'].tolist(), dtype = torch.long)\n",
    "\n",
    "        # Choose the appropriate batch size\n",
    "        batch_size = self.train_batch_size if is_train else self.test_batch_size\n",
    "\n",
    "        # Create dataset and dataloader\n",
    "        dataset = TensorDataset(input_ids, attention_mask, labels, ids)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n",
    "\n",
    "        return dataloader, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085aab8-4c3b-4938-b736-52bf554f7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "\n",
    "preprocessor = DataPreprocessor(tokenizer, label2id)\n",
    "train_dataloader, train_seq_lengths = preprocessor.prepare(train, is_train=True)\n",
    "test_dataloader, test_seq_lengths = preprocessor.prepare(test, is_train=False)\n",
    "\n",
    "time1 = time.time()\n",
    "print(f\"Tokenization duration: {time1-time0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4339f-19b7-41a7-b722-82b33512101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average length of sequences in train: {sum(train_seq_lengths) / len(train_seq_lengths):.0f}\")\n",
    "print(f\"Average length of sequences in test: {sum(test_seq_lengths) / len(test_seq_lengths):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edfe8ea-2e06-4a1e-8ddd-3ffdd144f1fc",
   "metadata": {},
   "source": [
    "After tokenization and dataloading, the average length of sequences in 512. This indicates that there is at least one sequences longer than 512 tokens in each batch, so that `padding = 'longest'` has the same effect as `padding = 'max_length'`. With a `batch_size` of 256, there are exactly $256×512=131,072$ tokens per (training) batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4f1ac-246b-4859-8755-03dbf37834eb",
   "metadata": {},
   "source": [
    "# 3. Training\n",
    "\n",
    "We define functions for **training** and **testing**. Since we plan to evaluate the model on the entire test dataset after each epoch, we store the logits returned at that time. This allows us to retrieve predictions directly, avoiding the need to recompute them. Additionally, we save the true labels and review IDs to link the predictions with the original dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81efe6f-e9e9-4069-b33a-40ea8e687d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67fdab-6373-49aa-8ae6-abacf58788f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train_losses = []\n",
    "batch_test_losses = []\n",
    "\n",
    "def model_train(batch):\n",
    "    b_input_ids, b_attention_mask, b_labels, _ = [t.to(device) for t in batch]\n",
    "        \n",
    "    # Reset gradients before backpropagation\n",
    "    roberta_classifier.zero_grad()\n",
    "        \n",
    "    # Perform a forward pass to calculate outputs\n",
    "    outputs = roberta_classifier(b_input_ids, b_attention_mask)\n",
    "        \n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(outputs, b_labels)\n",
    "    batch_train_losses.append(loss.item())\n",
    "        \n",
    "    # Backpropagate the loss\n",
    "    loss.backward()\n",
    "        \n",
    "    # Update model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfce21a-d6cb-4854-b7db-5cd307713288",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "\n",
    "def model_eval(batch):\n",
    "    b_input_ids, b_attention_mask, b_labels, b_ids = [t.to(device) for t in batch]\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = roberta_classifier(b_input_ids, b_attention_mask)\n",
    "\n",
    "    # Store results \n",
    "    all_logits.append(logits.detach().cpu())  # Logits\n",
    "    all_labels.append(b_labels.detach().cpu())  # True labels\n",
    "    all_ids.append(b_ids.detach().cpu())  # Reviews ID\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = loss_fn(logits, b_labels)\n",
    "    batch_test_losses.append(loss.item())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae023e-45a5-4d3c-9a7d-b866d91c9fc4",
   "metadata": {},
   "source": [
    "We define a **training loop** with the ability to resume after interruption.\n",
    "\n",
    "The training dataset is processed `num_epochs` times. Typical values for fine-tuning range between 3 to 12. We set it to 10 and will compute the test error after every batch to detect when the model starts overfitting.\n",
    "\n",
    "For the **optimizer**, we set a relatively low learning rate, as is commonly recommended for fine-tuning RoBERTa models. The `weight_decay` parameter is a regularization term that adds a penalty for large weights during optimization, helping to mitigate overfitting.\n",
    "\n",
    "We use a **scheduler** to further reduce the learning rate as the model converges. The `num_warmup_steps` is another regularization parameter, which gradually reduces the learning rate during the first batches of each epoch to prevent instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ce885-1fc1-4e91-acbe-15ff237d8fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 12\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "print(f\"Number of training steps: {num_training_steps}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    roberta_classifier.parameters(), \n",
    "    lr = 5e-4, \n",
    "    weight_decay = 0.01)\n",
    "\n",
    "# Scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer = optimizer,\n",
    "    num_warmup_steps = 0.05 * num_training_steps,\n",
    "    num_training_steps = num_training_steps)\n",
    "\n",
    "# Avoid issues with multithreading\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da1f41-30ea-4e1b-9706-74390dde444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training\n",
    "def get_latest_checkpoint(model_dir):\n",
    "    # Verify if checkpoints are present\n",
    "    checkpoint_files = glob.glob(os.path.join(model_dir, \"epoch_*.pth\"))\n",
    "    if not checkpoint_files:\n",
    "        return None, 0\n",
    "\n",
    "    # Extract epoch numbers from filenames\n",
    "    epochs = [int(os.path.basename(f).split(\"_\")[1].split(\".\")[0]) for f in checkpoint_files]\n",
    "    last_epoch = max(epochs)\n",
    "    \n",
    "    return os.path.join(model_dir, f\"epoch_{last_epoch}.pth\"), last_epoch\n",
    "\n",
    "latest_checkpoint, last_epoch = get_latest_checkpoint(\"output/models\")\n",
    "\n",
    "if latest_checkpoint:\n",
    "    if last_epoch >= num_epochs:\n",
    "        print(f\"Training already completed up to epoch {last_epoch}.\")\n",
    "        \n",
    "    else:\n",
    "        checkpoint = torch.load(latest_checkpoint, weights_only = False)     \n",
    "        roberta_classifier.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "\n",
    "        batch_train_losses = checkpoint['batch_train_losses']\n",
    "        epoch_train_losses = checkpoint['epoch_train_losses']\n",
    "        batch_test_losses = checkpoint['batch_test_losses']\n",
    "        epoch_train_losses = checkpoint['epoch_train_losses']\n",
    "\n",
    "        batch_times_train = checkpoint['batch_times_train']\n",
    "        batch_times_test = checkpoint['batch_times_test']\n",
    "        epoch_times = checkpoint['epoch_times']\n",
    "\n",
    "        print(f\"Resuming from epoch {last_epoch}\")\n",
    "\n",
    "else:\n",
    "    # Computation time tracking\n",
    "    batch_times_train = []\n",
    "    batch_times_test = []\n",
    "    epoch_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c38e0b-625a-46c1-80f7-25230260e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory tracking\n",
    "if device == 'mps':\n",
    "    torch.mps.empty_cache()\n",
    "baseline_memory = psutil.virtual_memory().available\n",
    "max_memory_test = 0\n",
    "max_memory_train = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c76e2b-3bed-4384-ac82-a448775b6880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_progress = tqdm(range(last_epoch, num_epochs),\n",
    "                      desc = \"Epochs\",\n",
    "                      position = 0,\n",
    "                      unit = \" epoch\")\n",
    "\n",
    "for epoch in epoch_progress:\n",
    "    epoch_st = time.time()\n",
    "    \n",
    "    ##### Training phase #####\n",
    "    # Set up the train progress bar\n",
    "    train_progress = tqdm(total = len(train_dataloader), \n",
    "                          desc = f\"Train\", \n",
    "                          position = 1, \n",
    "                          leave = False,\n",
    "                          unit = \" batch\")\n",
    "    \n",
    "    roberta_classifier.train()\n",
    "    for batch in train_dataloader:\n",
    "        batch_st = time.time()\n",
    "        loss = model_train(batch)\n",
    "        # Update progress bar with current loss\n",
    "        train_progress.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        train_progress.update(1)\n",
    "        # Track memory use and computation time\n",
    "        max_memory_train = max(max_memory_train, psutil.virtual_memory().available)\n",
    "        batch_times_train.append(time.time() - batch_st)\n",
    "    \n",
    "    train_progress.close()\n",
    "    # Calculate average train loss over the epoch\n",
    "    avg_train_loss = np.mean(batch_train_losses[-len(train_dataloader):])\n",
    "    epoch_train_losses.append(avg_train_loss)\n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    \n",
    "    ##### Testing phase #####\n",
    "    # Set up the test progress bar\n",
    "    test_progress = tqdm(total = len(test_dataloader), \n",
    "                         desc = f\"Test\", \n",
    "                         position = 2, \n",
    "                         leave = False,\n",
    "                         unit = \" batch\")\n",
    "    \n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    all_ids = []\n",
    "    \n",
    "    roberta_classifier.eval()\n",
    "    for batch in test_dataloader:\n",
    "        batch_st = time.time()\n",
    "        loss = model_eval(batch)\n",
    "        # Update progress bar with current loss\n",
    "        test_progress.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        test_progress.update(1)\n",
    "        # Track memory use and computation time\n",
    "        max_memory_test = max(max_memory_test, psutil.virtual_memory().available)\n",
    "        batch_times_test.append(time.time() - batch_st)\n",
    "    \n",
    "    test_progress.close()\n",
    "    # Calculate average test loss over the epoch\n",
    "    avg_test_loss = np.mean(batch_test_losses[-len(test_dataloader):])\n",
    "    epoch_test_losses.append(avg_test_loss)\n",
    "    \n",
    "    \n",
    "    ##### Closing phase #####\n",
    "    # Update epoch progress bar with average losses\n",
    "    epoch_progress.set_postfix(train_loss=f\"{avg_train_loss:.4f}\", test_loss=f\"{avg_test_loss:.4f}\")\n",
    " \n",
    "    # Save predictions\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    all_ids = torch.cat(all_ids, dim=0)\n",
    "    \n",
    "    probs = F.softmax(all_logits, dim=1) # Get probabilities\n",
    "    probs_array = probs.detach().cpu().numpy()\n",
    "    labels_array = all_labels.detach().cpu().numpy()\n",
    "    ids_array = all_ids.detach().cpu().numpy()\n",
    "\n",
    "    results = pd.DataFrame(probs_array, columns=[f\"prob_class_{i}\" for i in range(probs_array.shape[1])])\n",
    "    results['true_label'] = [id2label[label] for label in labels_array]\n",
    "    results['review_id'] = ids_array\n",
    "    results.to_csv(f\"output/preds/epoch_{epoch+1}.csv\", index=False)\n",
    "\n",
    "    # Track computation time\n",
    "    epoch_times.append(time.time() - epoch_st)\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    model_save_path = os.path.join(\"output/models\", f\"epoch_{epoch+1}.pth\")\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model': roberta_classifier.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'batch_train_losses': batch_train_losses,\n",
    "        'epoch_train_losses': epoch_train_losses,\n",
    "        'batch_test_losses': batch_test_losses,\n",
    "        'epoch_test_losses': epoch_test_losses,\n",
    "        'batch_times_train': batch_times_train,\n",
    "        'batch_times_test': batch_times_test,\n",
    "        'epoch_times': epoch_times\n",
    "    }\n",
    "    torch.save(checkpoint, model_save_path)\n",
    "\n",
    "    # Clean memory\n",
    "    if device == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "epoch_progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4120880-6937-41d1-94b4-19dd3f82d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max memory used for training is {(baseline_memory - max_memory_train) / 1e9:.2f} GB\")\n",
    "print(f\"Max memory used for evaluating is {(baseline_memory - max_memory_test) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9e844-378c-4e44-8882-17bf025e7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average computation time for training batches: {np.mean(batch_times_train):.2f} seconds\")\n",
    "print(f\"Average computation time for validation batches: {np.mean(batch_times_test):.2f} seconds\")\n",
    "print(f\"Average computation time per epoch: {np.mean(epoch_times):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5f1cf-85fa-4dff-a636-3e5a962846a0",
   "metadata": {},
   "source": [
    "# 3. Results\n",
    "## 1. Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b494ae92-815c-47b5-bfed-bba7cec74fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot batch loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batch_train_losses, label='Train', color='cornflowerblue')\n",
    "# Match the scale for longer test batches\n",
    "test_x = np.linspace(0, len(batch_train_losses) - 1, len(batch_test_losses))\n",
    "plt.plot(test_x, batch_test_losses, label='Test', color='goldenrod')\n",
    "plt.xlabel('Batches')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Batch Loss Over Training')\n",
    "plt.legend()\n",
    "\n",
    "# Plot epoch loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_train_losses, label='Train', color='cornflowerblue')\n",
    "plt.plot(epoch_test_losses, label='Test', color='goldenrod')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('')\n",
    "plt.title('Epoch Loss Over Training')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output/RoBERTa_learning_curve.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe429ee-df32-446d-8b5b-6c540f8bec58",
   "metadata": {},
   "source": [
    "## 2. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37803a-ca22-4069-b10f-ad34e3ca940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read predictions from the best model and merge them with the original dataset\n",
    "results = pd.read_csv(\"output/preds/epoch_10.csv\")\n",
    "results = pd.merge(test, results, on = 'review_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f086402-ca65-40b4-9f6d-596f19aebb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for consistency\n",
    "print(\"Do the true labels returned by the model match the original sentiments?\")\n",
    "print(\"Yes!\" if (results['sentiment'] == results['true_label']).all() else \"No :'(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9893a02-b561-414f-b431-cfd257c786c1",
   "metadata": {},
   "source": [
    "### Assess certainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ddc42-405e-4045-9fc8-4627667b4f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of ties: {(results['prob_class_0'] == results['prob_class_1']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8143025-1c5d-4d9f-8ded-ffca4d11b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = (1 - np.maximum(results['prob_class_0'], results['prob_class_1'])).mean()\n",
    "print(f\"Classification error {E:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece83f3-8043-416f-bcc7-7710606ac1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(results['prob_class_1'], bins=20, color=\"cornflowerblue\", kde=True, stat=\"probability\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Share of reviews\")\n",
    "plt.title(\"Distribution of Predicted Probabilities for Positive Sentiment\")\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x * 100:.1f}%'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080997c8-6511-46a6-a296-3b108e1c8847",
   "metadata": {},
   "source": [
    "### Get predicted sentiments and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa6d97-8e3f-4e09-8fe1-c0b71136b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['RoBERTa_ft'] = np.where(results['prob_class_1'] >= 0.5, 'positive', 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc866af5-c8c8-41e4-908a-6b6058b3fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['review_id', 'RoBERTa_ft']].to_csv(\"output/RoBERTa_ft.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25586764-32dc-4511-9c9e-1ae5ff6539fa",
   "metadata": {},
   "source": [
    "### Assess performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0f822-71f7-4f6f-affb-b7d870ccd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(results['sentiment'], results['RoBERTa_ft'])\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2678c49-30df-43d5-af3f-41ff0a295710",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
