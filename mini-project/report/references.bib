
@article{munezero_are_2014,
	title = {Are {They} {Different}? {Affect}, {Feeling}, {Emotion}, {Sentiment}, and {Opinion} {Detection} in {Text}},
	volume = {5},
	issn = {1949-3045},
	shorttitle = {Are {They} {Different}?},
	url = {https://ieeexplore.ieee.org/document/6797872},
	doi = {10.1109/TAFFC.2014.2317187},
	abstract = {A major limitation in the automatic detection of affect, feelings, emotions, sentiments, and opinions in text is the lack of proper differentiation between these subjective terms and understanding of how they relate to one another. This lack of differentiation not only leads to inconsistency in terminology usage but also makes the subtleties and nuances expressed by the five terms difficult to understand, resulting in subpar detection of the terms in text. In light of such limitation, this paper clarifies the differences between these five subjective terms and reveals significant concepts to the computational linguistics community for their effective detection and processing in text.},
	number = {2},
	urldate = {2025-03-25},
	journal = {IEEE Transactions on Affective Computing},
	author = {Munezero, Myriam and Montero, Calkin Suero and Sutinen, Erkki and Pajunen, John},
	month = apr,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Affective Computing},
	keywords = {Appraisal, Psychology, Affect sensing and analysis, computation models of emotion, Cultural differences, Educational institutions, Electronic mail, Natural language processing, Physiology},
	pages = {101--111},
}

@article{stine_sentiment_2019,
	title = {Sentiment {Analysis}},
	volume = {6},
	issn = {2326-8298, 2326-831X},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-030718-105242},
	doi = {10.1146/annurev-statistics-030718-105242},
	abstract = {Sentiment analysis labels a body of text as expressing either a positive or negative opinion, as in summarizing the content of an online product review. In this sense, sentiment analysis can be considered the challenge of building a classifier from text. Sentiment analysis can be done by counting the words from a dictionary of emotional terms, by fitting traditional classifiers such as logistic regression to word counts, or, most recently, by employing sophisticated neural networks. These methods progressively improve classification at the cost of increased computation and reduced transparency. A common sentiment analysis task, the classification of IMDb (Internet Movie Database) movie reviews, is used to illustrate the methods on a common task that appears frequently in the literature.},
	language = {fr},
	urldate = {2025-03-25},
	journal = {Annual Review of Statistics and Its Application},
	author = {Stine, Robert A.},
	month = mar,
	year = {2019},
	note = {Publisher: Annual Reviews},
	pages = {287--308},
}

@article{ravi_survey_2015,
	title = {A survey on opinion mining and sentiment analysis: {Tasks}, approaches and applications},
	volume = {89},
	issn = {0950-7051},
	shorttitle = {A survey on opinion mining and sentiment analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705115002336},
	doi = {10.1016/j.knosys.2015.06.015},
	abstract = {With the advent of Web 2.0, people became more eager to express and share their opinions on web regarding day-to-day activities and global issues as well. Evolution of social media has also contributed immensely to these activities, thereby providing us a transparent platform to share views across the world. These electronic Word of Mouth (eWOM) statements expressed on the web are much prevalent in business and service industry to enable customer to share his/her point of view. In the last one and half decades, research communities, academia, public and service industries are working rigorously on sentiment analysis, also known as, opinion mining, to extract and analyze public mood and views. In this regard, this paper presents a rigorous survey on sentiment analysis, which portrays views presented by over one hundred articles published in the last decade regarding necessary tasks, approaches, and applications of sentiment analysis. Several sub-tasks need to be performed for sentiment analysis which in turn can be accomplished using various approaches and techniques. This survey covering published literature during 2002–2015, is organized on the basis of sub-tasks to be performed, machine learning and natural language processing techniques used and applications of sentiment analysis. The paper also presents open issues and along with a summary table of a hundred and sixty-one articles.},
	urldate = {2025-03-25},
	journal = {Knowledge-Based Systems},
	author = {Ravi, Kumar and Ravi, Vadlamani},
	month = nov,
	year = {2015},
	keywords = {Social media, Machine learning, Lexica creation, Micro blog, Ontology, Opinion mining, Sentiment analysis},
	pages = {14--46},
}

@article{hemmatian_survey_2019,
	title = {A survey on classification techniques for opinion mining and sentiment analysis},
	volume = {52},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-017-9599-6},
	doi = {10.1007/s10462-017-9599-6},
	abstract = {Opinion mining is considered as a subfield of natural language processing, information retrieval and text mining. Opinion mining is the process of extracting human thoughts and perceptions from unstructured texts, which with regard to the emergence of online social media and mass volume of users’ comments, has become to a useful, attractive and also challenging issue. There are varieties of researches with different trends and approaches in this area, but the lack of a comprehensive study to investigate them from all aspects is tangible. In this paper we represent a complete, multilateral and systematic review of opinion mining and sentiment analysis to classify available methods and compare their advantages and drawbacks, in order to have better understanding of available challenges and solutions to clarify the future direction. For this purpose, we present a proper framework of opinion mining accompanying with its steps and levels and then we completely monitor, classify, summarize and compare proposed techniques for aspect extraction, opinion classification, summary production and evaluation, based on the major validated scientific works. In order to have a better comparison, we also propose some factors in each category, which help to have a better understanding of advantages and disadvantages of different methods.},
	language = {en},
	number = {3},
	urldate = {2025-03-25},
	journal = {Artificial Intelligence Review},
	author = {Hemmatian, Fatemeh and Sohrabi, Mohammad Karim},
	month = oct,
	year = {2019},
	keywords = {Classification, Machine learning, Opinion mining, Sentiment analysis, Artificial Intelligence, Lexicon},
	pages = {1495--1545},
}

@misc{kumar_comprehensive_2023,
	title = {A {Comprehensive} {Review} on {Sentiment} {Analysis}: {Tasks}, {Approaches} and {Applications}},
	shorttitle = {A {Comprehensive} {Review} on {Sentiment} {Analysis}},
	url = {http://arxiv.org/abs/2311.11250},
	doi = {10.48550/arXiv.2311.11250},
	abstract = {Sentiment analysis (SA) is an emerging field in text mining. It is the process of computationally identifying and categorizing opinions expressed in a piece of text over different social media platforms. Social media plays an essential role in knowing the customer mindset towards a product, services, and the latest market trends. Most organizations depend on the customer's response and feedback to upgrade their offered products and services. SA or opinion mining seems to be a promising research area for various domains. It plays a vital role in analyzing big data generated daily in structured and unstructured formats over the internet. This survey paper defines sentiment and its recent research and development in different domains, including voice, images, videos, and text. The challenges and opportunities of sentiment analysis are also discussed in the paper. {\textbackslash}keywords\{Sentiment Analysis, Machine Learning, Lexicon-based approach, Deep Learning, Natural Language Processing\}},
	urldate = {2025-03-25},
	publisher = {arXiv},
	author = {Kumar, Sudhanshu and Roy, Partha Pratim and Dogra, Debi Prosad and Kim, Byung-Gyu},
	month = nov,
	year = {2023},
	note = {arXiv:2311.11250 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{hartmann_more_2023,
	title = {More than a {Feeling}: {Accuracy} and {Application} of {Sentiment} {Analysis}},
	volume = {40},
	issn = {0167-8116},
	shorttitle = {More than a {Feeling}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167811622000477},
	doi = {10.1016/j.ijresmar.2022.05.005},
	abstract = {Sentiment is fundamental to human communication. Countless marketing applications mine opinions from social media communication, news articles, customer feedback, or corporate communication. Various sentiment analysis methods are available and new ones have recently been proposed. Lexicons can relate individual words and expressions to sentiment scores. In contrast, machine learning methods are more complex to interpret, but promise higher accuracy, i.e., fewer false classifications. We propose an empirical framework and quantify these trade-offs for different types of research questions, data characteristics, and analytical resources to enable informed method decisions contingent on the application context. Based on a meta-analysis of 272 datasets and 12 million sentiment-labeled text documents, we find that the recently proposed transfer learning models indeed perform best, but can perform worse than popular leaderboard benchmarks suggest. We quantify the accuracy-interpretability trade-off, showing that, compared to widely established lexicons, transfer learning models on average classify more than 20 percentage points more documents correctly. To form realistic performance expectations, additional context variables, most importantly the desired number of sentiment classes and the text length, should be taken into account. We provide a pre-trained sentiment analysis model (called SiEBERT) with open-source scripts that can be applied as easily as an off-the-shelf lexicon.},
	number = {1},
	urldate = {2025-03-25},
	journal = {International Journal of Research in Marketing},
	author = {Hartmann, Jochen and Heitmann, Mark and Siebert, Christian and Schamp, Christina},
	month = mar,
	year = {2023},
	keywords = {Natural Language Processing, Deep Contextual Language Models, Machine Learning, Meta-Analysis, Sentiment Analysis, Text Mining, Transfer Learning},
	pages = {75--87},
}

@article{hutto_vader_2014,
	title = {{VADER}: {A} {Parsimonious} {Rule}-{Based} {Model} for {Sentiment} {Analysis} of {Social} {Media} {Text}},
	volume = {8},
	copyright = {Copyright (c) 2021 Proceedings of the International AAAI Conference on Web and Social Media},
	issn = {2334-0770},
	shorttitle = {{VADER}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/14550},
	doi = {10.1609/icwsm.v8i1.14550},
	abstract = {The inherent nature of social media content poses serious challenges to practical applications of sentiment analysis. We present VADER, a simple rule-based model for general sentiment analysis, and compare its effectiveness to eleven typical state-of-practice benchmarks including LIWC, ANEW, the General Inquirer, SentiWordNet, and machine learning oriented techniques relying on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) algorithms. Using a combination of qualitative and quantitative methods, we first construct and empirically validate a gold-standard list of lexical features (along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like contexts. We then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity. Interestingly, using our parsimonious rule-based model to assess the sentiment of tweets, we find that VADER outperforms individual human raters (F1 Classification Accuracy = 0.96 and 0.84, respectively), and generalizes more favorably across contexts than any of our benchmarks.},
	language = {en},
	number = {1},
	urldate = {2025-03-25},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Hutto, C. and Gilbert, Eric},
	month = may,
	year = {2014},
	note = {Number: 1},
	keywords = {Human Centered Computing},
	pages = {216--225},
}

@inproceedings{wang_sentiment_2023,
	title = {Sentiment {Analysis} {Using} {Support} {Vector} {Machines}, {Neural} {Networks}, and {Random} {Forests}},
	isbn = {978-94-6463-300-9},
	url = {https://www.atlantis-press.com/proceedings/iciaai-23/125994591},
	doi = {10.2991/978-94-6463-300-9_4},
	abstract = {Sentiment analysis, often referred to as opinion mining, plays a crucial role in the field of natural language processing by computationally analyzing text to extract sentiments or opinions. It has become essential in understanding public sentiment, evaluating customer feedback, and identifying market trends, particularly in the age of online platforms...},
	language = {en},
	urldate = {2025-03-25},
	publisher = {Atlantis Press},
	author = {Wang, Colin Kai},
	month = nov,
	year = {2023},
	note = {ISSN: 2352-538X},
	pages = {23--34},
}

@misc{radford_improving_2018,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	url = {https://openai.com/index/language-unsupervised/},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	month = jun,
	year = {2018},
}

@misc{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	url = {https://openai.com/index/better-language-models/},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	month = feb,
	year = {2019},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-03-25},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{ghatora_sentiment_2024,
	title = {Sentiment {Analysis} of {Product} {Reviews} {Using} {Machine} {Learning} and {Pre}-{Trained} {LLM}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-2289},
	url = {https://www.mdpi.com/2504-2289/8/12/199},
	doi = {10.3390/bdcc8120199},
	abstract = {Sentiment analysis via artificial intelligence, i.e., machine learning and large language models (LLMs), is a pivotal tool that classifies sentiments within texts as positive, negative, or neutral. It enables computers to automatically detect and interpret emotions from textual data, covering a spectrum of feelings without direct human intervention. Sentiment analysis is integral to marketing research, helping to gauge consumer emotions and opinions across various sectors. Its applications span analyzing movie reviews, monitoring social media, evaluating product feedback, assessing employee sentiments, and identifying hate speech. This study explores the application of both traditional machine learning and pre-trained LLMs for automated sentiment analysis of customer product reviews. The motivation behind this work lies in the demand for more nuanced understanding of consumer sentiments that can drive data-informed business decisions. In this research, we applied machine learning-based classifiers, i.e., Random Forest, Naive Bayes, and Support Vector Machine, alongside the GPT-4 model to benchmark their effectiveness for sentiment analysis. Traditional models show better results and efficiency in processing short, concise text, with SVM in classifying sentiment of short length comments. However, GPT-4 showed better results with more detailed texts, capturing subtle sentiments with higher precision, recall, and F1 scores to uniquely identify mixed sentiments not found in the simpler models. Conclusively, this study shows that LLMs outperform traditional models in context-rich sentiment analysis by not only providing accurate sentiment classification but also insightful explanations. These results enable LLMs to provide a superior tool for customer-centric businesses, which helps actionable insights to be derived from any textual data.},
	language = {en},
	number = {12},
	urldate = {2025-03-25},
	journal = {Big Data and Cognitive Computing},
	author = {Ghatora, Pawanjit Singh and Hosseini, Seyed Ebrahim and Pervez, Shahbaz and Iqbal, Muhammad Javed and Shaukat, Nabil},
	month = dec,
	year = {2024},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {GPT-4, large language models (LLMs), machine learning, OpenAI, product reviews, sentiment analysis, text analysis},
	pages = {199},
}

@inproceedings{maas_learning_2011,
	address = {Portland, Oregon, USA},
	title = {Learning {Word} {Vectors} for {Sentiment} {Analysis}},
	url = {https://aclanthology.org/P11-1015/},
	urldate = {2025-03-25},
	booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
	editor = {Lin, Dekang and Matsumoto, Yuji and Mihalcea, Rada},
	month = jun,
	year = {2011},
	pages = {142--150},
}

@misc{csanady_llambert_2024,
	title = {{LlamBERT}: {Large}-scale low-cost data annotation in {NLP}},
	shorttitle = {{LlamBERT}},
	url = {http://arxiv.org/abs/2403.15938},
	doi = {10.48550/arXiv.2403.15938},
	abstract = {Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks. Despite their effectiveness, the high costs associated with their use pose a challenge. We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small subset of large, unlabeled databases and uses the results for fine-tuning transformer encoders like BERT and RoBERTa. This strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness.},
	urldate = {2025-03-25},
	publisher = {arXiv},
	author = {Csanády, Bálint and Muzsai, Lajos and Vedres, Péter and Nádasdy, Zoltán and Lukács, András},
	month = mar,
	year = {2024},
	note = {arXiv:2403.15938 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{yang_xlnet_2020,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	shorttitle = {{XLNet}},
	url = {http://arxiv.org/abs/1906.08237},
	doi = {10.48550/arXiv.1906.08237},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
	urldate = {2025-03-25},
	publisher = {arXiv},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	month = jan,
	year = {2020},
	note = {arXiv:1906.08237 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2025-03-25},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{warner_smarter_2024,
	title = {Smarter, {Better}, {Faster}, {Longer}: {A} {Modern} {Bidirectional} {Encoder} for {Fast}, {Memory} {Efficient}, and {Long} {Context} {Finetuning} and {Inference}},
	shorttitle = {Smarter, {Better}, {Faster}, {Longer}},
	url = {http://arxiv.org/abs/2412.13663},
	doi = {10.48550/arXiv.2412.13663},
	abstract = {Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.},
	urldate = {2025-03-25},
	publisher = {arXiv},
	author = {Warner, Benjamin and Chaffin, Antoine and Clavié, Benjamin and Weller, Orion and Hallström, Oskar and Taghadouini, Said and Gallagher, Alexis and Biswas, Raja and Ladhak, Faisal and Aarsen, Tom and Cooper, Nathan and Adams, Griffin and Howard, Jeremy and Poli, Iacopo},
	month = dec,
	year = {2024},
	note = {arXiv:2412.13663 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{wang_is_2024,
	title = {Is {ChatGPT} a {Good} {Sentiment} {Analyzer}? {A} {Preliminary} {Study}},
	shorttitle = {Is {ChatGPT} a {Good} {Sentiment} {Analyzer}?},
	url = {http://arxiv.org/abs/2304.04339},
	doi = {10.48550/arXiv.2304.04339},
	abstract = {Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly interested in whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of {\textbackslash}emph\{opinions\}, {\textbackslash}emph\{sentiments\}, and {\textbackslash}emph\{emotions\} contained in the text. Specifically, we evaluate it in three settings, including {\textbackslash}emph\{standard\} evaluation, {\textbackslash}emph\{polarity shift\} evaluation and {\textbackslash}emph\{open-domain\} evaluation. We conduct an evaluation on 7 representative sentiment analysis tasks covering 17 benchmark datasets and compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on them. We also attempt several popular prompting techniques to elicit the ability further. Moreover, we conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.},
	urldate = {2025-03-25},
	publisher = {arXiv},
	author = {Wang, Zengzhi and Xie, Qiming and Feng, Yi and Ding, Zixiang and Yang, Zinong and Xia, Rui},
	month = feb,
	year = {2024},
	note = {arXiv:2304.04339 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2025-03-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}
