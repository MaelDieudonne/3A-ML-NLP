{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2ecc8-6384-471b-964d-f1a536724bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28bb310-448d-459d-8c2b-d8983d35f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
    "else: device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4823e-ef96-4b8f-afea-fbcbc73127d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"output/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39025866-c7e2-42e0-a78e-026e977f62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c9dc2-6600-411e-8eaf-b4ac5782d368",
   "metadata": {},
   "source": [
    "RoBERTa has a context length of 512 tokens. The Hugging Face pipeline does not support automatic truncation of longer sequences. A simple workaround is to truncate the text before classification, but since tokenization occurs at the subword level, the exact truncation point is unpredictable. To avoid exceeding the limit, it's necessary to pass shorter sequences to the model than it could process (e.g., 1500 characters, that could correspond to 450 tokens in average).\n",
    "\n",
    "A more precise approach is to tokenize the text and then truncate it. However, since the pipeline does not accept tokenized input, the truncated text must be decoded before classification. This process is inefficient, as it requires encoding the text twice, but allows to make use of the full model capacity.\n",
    "\n",
    "We also perform truncation from the left, to preserve summaries that sometimes appear at the end of reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faf3cb6-c4db-4309-ab26-fd72ea7319cf",
   "metadata": {},
   "source": [
    "# Zero-shot classification with RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ba49d-b760-4bbf-88f1-fcd25a271fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer separately and set truncation strategy\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "# Pass this tokenizer to the pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model = \"roberta-large-mnli\",\n",
    "                      tokenizer = tokenizer,\n",
    "                      framework = \"pt\",\n",
    "                      device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1969dd0b-484d-4c6b-803e-0258a2ceea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_set = [\n",
    "    [\"positive\", \"negative\"],\n",
    "    [\"positive sentiment\", \"negative sentiment\"],\n",
    "    [\"positive review\", \"negative review\"],\n",
    "    [\"favorable opinion\", \"unfavorable opinion\"],\n",
    "    [\"good movie\", \"bad movie\"],\n",
    "    [\"excellent\", \"terrible\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5edebb5-1033-4f34-b2af-34b6214a9bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment(text, labels):\n",
    "    result = classifier(text, \n",
    "                        candidate_labels = labels, \n",
    "                        truncation = True)\n",
    "    return result[\"labels\"][0]\n",
    "\n",
    "models = []\n",
    "for _, labels in enumerate(labels_set):\n",
    "    # Create a column name based on the first label in each set\n",
    "    column_name = f\"RoBERTa_{labels[0].replace(' ', '_')}\"\n",
    "    models.append(column_name)\n",
    "    \n",
    "    # Apply the sentiment analysis with the current set of labels\n",
    "    tqdm.pandas(desc=f\"Processing with {labels}\", unit = \" reviews\")\n",
    "    test[column_name] = test[\"text\"].progress_apply(lambda x: get_sentiment(x, labels))\n",
    "\n",
    "    # Standardize labels\n",
    "    mapping = {labels[0]: \"positive\", \n",
    "               labels[1]: \"negative\"}\n",
    "    test[column_name] = test[column_name].replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f873d92-a96f-4a1d-a381-9b587654e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {model: accuracy_score(test[\"sentiment\"], test[model]) for model in models}\n",
    "accuracy_avg = pd.DataFrame(accuracies.items(), columns=[\"Labels\", \"Accuracy\"])\n",
    "accuracy_avg.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc12b4e-6c64-40e5-b5f7-876e5eb7daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = accuracy_avg.loc[accuracy_avg['Accuracy'].idxmax(), 'Model']\n",
    "test = test.rename(columns={best_model: \"RoBERTa_base\"})\n",
    "test[['review_id', 'RoBERTa_base']].to_csv(\"output/RoBERTa_base.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053870f3-10e9-428f-a762-6ea7eb0e18bd",
   "metadata": {},
   "source": [
    "# Zero-shot classification with SiEBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486373ec-9c51-4c03-b667-c6697c4a93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer separately and set truncation strategy\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "tokenizer.truncation_side = \"left\"\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "# Pass the tokenizer to the pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\",\n",
    "                      model = \"siebert/sentiment-roberta-large-english\",\n",
    "                      tokenizer = tokenizer,\n",
    "                      framework = \"pt\",\n",
    "                      device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0f026-8c1b-40de-864a-509207c41506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    result = classifier(text, truncation=True)\n",
    "    return result[0][\"label\"].lower()\n",
    "\n",
    "tqdm.pandas(unit = \" reviews\")\n",
    "test[\"SiEBERT\"] = test[\"text\"].progress_apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b5cf6-c22c-4454-9480-3553381b4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['review_id', 'SiEBERT']].to_csv(\"output/SiEBERT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a7d6a-621b-4b72-99e3-e3ec5bd3631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(test['sentiment'], test['SiEBERT'])\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
