{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2ecc8-6384-471b-964d-f1a536724bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70424075-f9a7-457d-a4a7-e6812d29c770",
   "metadata": {},
   "source": [
    "# Merge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39025866-c7e2-42e0-a78e-026e977f62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db702ddb-51ba-4b51-bdbf-7bd4c8e9ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "RoBERTa_base = pd.read_csv(\"output/RoBERTa_base.csv\")\n",
    "results = pd.merge(results, RoBERTa_base, on = 'review_id')\n",
    "\n",
    "if 'Unnamed: 0' in results.columns: results = results.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d1aae-ff92-4c60-9bb3-a18e2a05995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RoBERTa_ft = pd.read_csv(\"output/RoBERTa_ft.csv\")\n",
    "results = pd.merge(results, RoBERTa_ft, on = 'review_id')\n",
    "\n",
    "if 'Unnamed: 0' in results.columns: results = results.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9cd83c-b772-4a50-8c50-6ed8e0df1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SiEBERT = pd.read_csv(\"output/SiEBERT.csv\")\n",
    "results = pd.merge(results, SiEBERT, on = 'review_id')\n",
    "\n",
    "if 'Unnamed: 0' in results.columns: results = results.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ca9758-1dc2-43fd-b0eb-594e1dc77980",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT = pd.read_csv(\"output/GPT.csv\")\n",
    "results = pd.merge(results, GPT[[\"review_id\", \"GPT\"]], on = 'review_id')\n",
    "\n",
    "if 'Unnamed: 0' in results.columns: results = results.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db0228-7937-4750-9574-d97bb8e3088f",
   "metadata": {},
   "source": [
    "# Compare performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf3ae0-8172-40e6-a1b9-35a7af44c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"RoBERTa_base\", \"RoBERTa_ft\", \"SiEBERT\", \"GPT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de52361-4590-4a71-8a83-040da7fe5df1",
   "metadata": {},
   "source": [
    "## Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b4bab-f40c-4384-8be6-4432c3440cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {model: accuracy_score(results[\"sentiment\"], results[model]) for model in models}\n",
    "accuracy_avg = pd.DataFrame(accuracies.items(), columns=[\"Model\", \"Accuracy\"])\n",
    "accuracy_avg.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6133364-570a-4282-941c-e255f98758cc",
   "metadata": {},
   "source": [
    "## By ratings / sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e3272-0e94-40f8-8e41-230e678a07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rtg = (\n",
    "    results.groupby(\"sentiment\")\n",
    "    .apply(lambda group: {model: accuracy_score([group.name] * len(group), group[model]) for model in models}, include_groups=False)\n",
    "    .apply(pd.Series)\n",
    "    .reset_index()\n",
    "    .set_index(\"sentiment\")\n",
    "    .T\n",
    "    .reset_index()\n",
    ")\n",
    "accuracy_rtg.rename(columns={\"index\": \"model\"}, inplace=True)\n",
    "accuracy_rtg[\"average\"] = accuracy_rtg.iloc[:, 1:].mean(axis=1)\n",
    "accuracy_rtg = accuracy_rtg.sort_values(by=\"average\", ascending=True)\n",
    "accuracy_rtg.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42099c0-98c6-4d38-bb48-e60141754a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_rtg = (\n",
    "    results.groupby(\"rating\")\n",
    "    .apply(lambda group: {model: accuracy_score(group[\"sentiment\"], group[model]) for model in models}, include_groups = False)\n",
    "    .apply(pd.Series)\n",
    ").sort_values(\"rating\").reset_index()\n",
    "accuracy_rtg.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490f775-d8bc-4697-95c5-d27d2983e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_long = accuracy_rtg.melt(id_vars=\"rating\", var_name=\"Model\", value_name=\"Accuracy\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Define a color map for the models\n",
    "colors = {\n",
    "    model: plt.cm.tab10(i) for i, model in enumerate(accuracy_long[\"Model\"].unique())\n",
    "}\n",
    "\n",
    "# Group by model and plot each group separately\n",
    "for model in accuracy_long[\"Model\"].unique():\n",
    "    model_data = accuracy_long[accuracy_long[\"Model\"] == model]\n",
    "    color = colors[model]\n",
    "    \n",
    "    # Split the data into two segments: ratings 1-4 and 7-10\n",
    "    lower_ratings = model_data[model_data[\"rating\"] <= 4]\n",
    "    higher_ratings = model_data[model_data[\"rating\"] >= 7]\n",
    "    \n",
    "    # Plot each segment with the same color\n",
    "    plt.plot(lower_ratings[\"rating\"], lower_ratings[\"Accuracy\"], marker=\"o\", linestyle=\"-\", \n",
    "             color=color, alpha=0.7, label=model if len(lower_ratings) > 0 else None)\n",
    "    plt.plot(higher_ratings[\"rating\"], higher_ratings[\"Accuracy\"], marker=\"o\", linestyle=\"-\", \n",
    "             color=color, alpha=0.7, label=None)\n",
    "    \n",
    "    # Connect the two segments with a styled line to indicate discontinuity if both segments exist\n",
    "    if len(lower_ratings) > 0 and len(higher_ratings) > 0:\n",
    "        plt.plot([lower_ratings[\"rating\"].iloc[-1], higher_ratings[\"rating\"].iloc[0]], \n",
    "                 [lower_ratings[\"Accuracy\"].iloc[-1], higher_ratings[\"Accuracy\"].iloc[0]], \n",
    "                 linestyle=\"--\", alpha=0.4, color=color)\n",
    "\n",
    "# Add a single legend entry for each model\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.savefig(\"output/accuracy_vs_ratings.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7415ba2-b9f8-47a7-b1fc-12930cbe46a1",
   "metadata": {},
   "source": [
    "## By review length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406cd72-8dc9-4167-bc94-f26ce7a275b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['nb_words'] = results['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "bins = pd.qcut(results[\"nb_words\"], q=10, duplicates=\"drop\")\n",
    "upper_bounds = np.array([interval.right for interval in bins.cat.categories])\n",
    "results[\"max_words\"] = upper_bounds[bins.cat.codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89037793-5913-446e-9e4b-e8f08b252ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_lth = (\n",
    "    results.groupby(\"max_words\")\n",
    "    .apply(lambda group: {model: accuracy_score(group[\"sentiment\"], group[model]) for model in models}, include_groups = False)\n",
    "    .apply(pd.Series)\n",
    ").reset_index()\n",
    "accuracy_lth.style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47603f46-5abc-484f-9a48-f814f345d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_long = accuracy_lth.melt(id_vars=\"max_words\", var_name=\"Model\", value_name=\"Accuracy\")\n",
    "accuracy_long[\"max_words\"] = accuracy_long[\"max_words\"].astype(float)\n",
    "accuracy_long = accuracy_long.sort_values(\"max_words\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=accuracy_long, x=\"max_words\", y=\"Accuracy\", hue=\"Model\", marker=\"o\", alpha=0.7)\n",
    "plt.xlabel(\"Review length in words\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"output/accuracy_vs_wordcount.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3519b6c-d212-4705-9601-90d61d41319e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
