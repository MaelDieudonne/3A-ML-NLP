{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68687f9f-dfc5-45ba-9bbc-9758e4b6e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39025866-c7e2-42e0-a78e-026e977f62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/processed/train.csv\")\n",
    "test = pd.read_csv(\"data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf978a95-539b-42e4-9414-448918bcedd4",
   "metadata": {},
   "source": [
    "# Create prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf3995b-e130-4b8c-a464-2fc7178db9c4",
   "metadata": {},
   "source": [
    "for prompting:\n",
    "\n",
    "Here is an example of a positive movie review, associated with a rating of 10:\n",
    "...\n",
    "\n",
    "Here is an example of a negative movie review, associated with a rating of 2:\n",
    "...\n",
    "\n",
    "What about the following review? Was its author feeling positive or negative about the movie? Answer with a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19af726-c4e9-4c11-8bab-8312c9225542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review length\n",
    "train['nb_words'] = train['sentiment'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b57f8bf-5ecf-4fe1-b58e-a7697d239676",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "for _, test_row in tqdm(test.iterrows(),\n",
    "                        total=test.shape[0],\n",
    "                        desc = \"Generating Prompts\",\n",
    "                        unit = \"prompts\"):\n",
    "    \n",
    "    # Select a random positive and negative sentiment review from train\n",
    "    positive_row = random.choice(\n",
    "        train[(train['sentiment'] == 'positive') & (train['nb_words'] < 100)].to_dict('records'))\n",
    "    negative_row = random.choice(\n",
    "        train[(train['sentiment'] == 'negative') & (train['nb_words'] < 100)].to_dict('records'))\n",
    "\n",
    "    # Randomize the order of positive and negative examples\n",
    "    examples = random.sample(\n",
    "        [(f\"Here is an example of a positive movie review, associated with a rating of {positive_row['rating']}:\\n{positive_row['text']}\",\n",
    "          f\"Here is an example of a negative movie review, associated with a rating of {negative_row['rating']}:\\n{negative_row['text']}\"),\n",
    "         (f\"Here is an example of a negative movie review, associated with a rating of {negative_row['rating']}:\\n{negative_row['text']}\",\n",
    "          f\"Here is an example of a positive movie review, associated with a rating of {positive_row['rating']}:\\n{positive_row['text']}\")\n",
    "        ], 1)[0]\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "    {examples[0]}\n",
    "\n",
    "    {examples[1]}\n",
    "\n",
    "    What about the following review? Was its author feeling positive or negative about the movie? Answer with a single word.\n",
    "    {test_row['text']}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts.append({\n",
    "        'review_id': test_row['review_id'],\n",
    "        'prompt': prompt\n",
    "    })\n",
    "    \n",
    "prompts = pd.DataFrame(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828cb26-f20f-4948-8b55-f09e671a5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "prompts.to_csv(\"data/processed/GTP_prompts.csv, index=False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca05f8d-49e3-4bed-8d6c-cf78f076a42d",
   "metadata": {},
   "source": [
    "# Estimate cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f520e-bfe4-4477-9c03-2578be076495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize with tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "prompts['tokens'] = prompts['prompt'].apply(lambda x: len(enc.encode(x)))\n",
    "total_tokens = prompts['tokens'].sum()\n",
    "print(f\"Total number of tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bf8e4-1eff-44ec-a5dc-2f511ce94c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(prompts['tokens'], bins=30, color='cornflowerblue', edgecolor='midnightblue')\n",
    "plt.title('Distribution of Token Counts')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f767e-ac19-4a6a-89a3-778cb141bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o = (total_tokens * 2.5 + len(prompts) * 1.25) * 1e-6\n",
    "gpt_35_turbo = (total_tokens * 0.5 + len(prompts) * 1.5) * 1e-6\n",
    "gpt_4o_mini = (total_tokens * 0.15 + len(prompts) * 0.075) * 1e-6\n",
    "\n",
    "print(f\"Cost with gpt-4o: {gpt_4o:.2f} $\")\n",
    "print(f\"Cost with gpt-3.5-turbo: {gpt_35_turbo:.2f} $\")\n",
    "print(f\"Cost with gpt-4o-mini: {gpt_4o_mini:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf350c-c4b7-4910-8518-c3d2b4dddb42",
   "metadata": {},
   "source": [
    "# Send prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c34e0-85c5-4677-bdd7-4c4852d9587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = pd.read_csv(\"data/processed/GTP_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dadd57-c71a-4b4f-aa64-0df245b5eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030df245-81d7-41d2-b18a-19fce702b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def get_openai_answer(review_id, df):\n",
    "    row = df[df['review_id'] == review_id]\n",
    "    prompt = row['prompt'].iloc[0]\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e} for review #{review_id}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a491048-5ddf-4d3e-9347-e4f1ae303821",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = get_openai_answer(1, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b725f-4f72-442c-815d-44a5e9dbc10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cce791-cdf2-498c-819a-8e1d8873f3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5630d5-8c2f-4580-af6f-4d48431565a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process in batches for prompts without answers\n",
    "# do not retry in case of failure\n",
    "# start over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae27876-9359-4c75-882d-c7bcb9270fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf3df6-6381-46ce-bf1d-2658c23bb09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d1cbc-a926-40e8-bd6e-6d2f184519c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc626613-4a83-46f3-80a9-15c725ed1edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550726c3-1b43-4a56-85a5-b8d0901e8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"output/GTP.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
