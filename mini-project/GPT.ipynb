{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68687f9f-dfc5-45ba-9bbc-9758e4b6e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39025866-c7e2-42e0-a78e-026e977f62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/processed/train.csv\")\n",
    "test = pd.read_csv(\"data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf978a95-539b-42e4-9414-448918bcedd4",
   "metadata": {},
   "source": [
    "# Create prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf3995b-e130-4b8c-a464-2fc7178db9c4",
   "metadata": {},
   "source": [
    "for prompting:\n",
    "\n",
    "Here is an example of a positive movie review, associated with a rating of 10:\n",
    "...\n",
    "\n",
    "Here is an example of a negative movie review, associated with a rating of 2:\n",
    "...\n",
    "\n",
    "What about the following review? Was its author feeling positive or negative about the movie? Answer with a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b57f8bf-5ecf-4fe1-b58e-a7697d239676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Prompts: 100%|██████████| 25000/25000 [14:44<00:00, 28.25it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = []\n",
    "\n",
    "# Iterate over the test dataset with a progress bar\n",
    "for _, test_row in tqdm(test.iterrows(), total=test.shape[0], desc=\"Generating Prompts\"):\n",
    "    # Select a random positive and negative sentiment review from train\n",
    "    positive_row = random.choice(train[train['sentiment'] == 'positive'].to_dict('records'))\n",
    "    negative_row = random.choice(train[train['sentiment'] == 'negative'].to_dict('records'))\n",
    "\n",
    "    # Randomize the order of positive and negative examples\n",
    "    examples = random.sample(\n",
    "        [(f\"Here is an example of a positive movie review, associated with a rating of {positive_row['rating']}:\\n{positive_row['text']}\",\n",
    "          f\"Here is an example of a negative movie review, associated with a rating of {negative_row['rating']}:\\n{negative_row['text']}\"),\n",
    "         (f\"Here is an example of a negative movie review, associated with a rating of {negative_row['rating']}:\\n{negative_row['text']}\",\n",
    "          f\"Here is an example of a positive movie review, associated with a rating of {positive_row['rating']}:\\n{positive_row['text']}\")\n",
    "        ], 1)[0]\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "    {examples[0]}\n",
    "\n",
    "    {examples[1]}\n",
    "\n",
    "    What about the following review? Was its author feeling positive or negative about the movie? Answer with a single word.\n",
    "    {test_row['text']}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts.append({\n",
    "        'review_id': test_row['review_id'],\n",
    "        'prompt': prompt\n",
    "    })\n",
    "    \n",
    "# Convert the list of prompts into a DataFrame\n",
    "prompts = pd.DataFrame(prompts)\n",
    "\n",
    "# Shuffle prompts to randomize the order of examples\n",
    "prompts = prompts.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b828cb26-f20f-4948-8b55-f09e671a5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts.to_csv(\"data/GTP_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df49e455-d749-482e-b7ca-8f175adcb2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of prompts length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf350c-c4b7-4910-8518-c3d2b4dddb42",
   "metadata": {},
   "source": [
    "# Send prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dadd57-c71a-4b4f-aa64-0df245b5eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2f901-7d14-4110-b446-e125ee704bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61719e-8c6e-44e4-9298-0b06269fed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6bf9c8-2e42-43b8-b459-f78077b29f5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "openai.api_key = 'your_openai_api_key'\n",
    "\n",
    "# Initialize a list to store responses\n",
    "responses = []\n",
    "\n",
    "# Iterate through each prompt and get the model's response, adding progress bar\n",
    "for index, row in tqdm(prompts_df.iterrows(), total=prompts_df.shape[0], desc=\"Processing Prompts\"):\n",
    "    prompt = row['prompt']\n",
    "\n",
    "    try:\n",
    "        # Send the prompt to the OpenAI API\n",
    "        response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",  # You can use other models like GPT-4 if you have access\n",
    "            prompt=prompt,\n",
    "            max_tokens=100,  # Adjust as needed\n",
    "            temperature=0.7,  # Adjust as needed\n",
    "            n=5,  # Generate 5 completions\n",
    "            stop=None\n",
    "        )\n",
    "        \n",
    "        # Collect the responses\n",
    "        answers = [choice.text.strip() for choice in response.choices]\n",
    "\n",
    "        # Store the response in the list\n",
    "        responses.append({\n",
    "            'review_id': row['review_id'],\n",
    "            'responses': answers  # A list of 5 different answers\n",
    "        })\n",
    "\n",
    "        # Optional: Sleep to prevent rate limiting issues\n",
    "        time.sleep(1)  # Adjust sleep time as needed based on rate limits\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with prompt {row['review_id']}: {e}\")\n",
    "        responses.append({\n",
    "            'review_id': row['review_id'],\n",
    "            'responses': None  # In case of an error, store None as the response\n",
    "        })\n",
    "\n",
    "# Convert responses to a DataFrame\n",
    "results = pd.DataFrame(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550726c3-1b43-4a56-85a5-b8d0901e8fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
