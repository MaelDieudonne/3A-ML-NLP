{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6198b68-e792-4ea6-b826-e78c870ecb6a",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68687f9f-dfc5-45ba-9bbc-9758e4b6e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed6cf3-5251-422f-aa16-343d9749b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"output/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d48dba-3e62-4bec-98b1-4233b5456310",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39025866-c7e2-42e0-a78e-026e977f62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/processed/train.csv\")\n",
    "test = pd.read_csv(\"data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf978a95-539b-42e4-9414-448918bcedd4",
   "metadata": {},
   "source": [
    "# Create prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf3995b-e130-4b8c-a464-2fc7178db9c4",
   "metadata": {},
   "source": [
    "Few-shots prompting, where prompts are constructed as follows:\n",
    "\n",
    "Here is an example of a positive movie review, associated with a rating of 10: \\*[ <...> ]\\*\n",
    "\n",
    "Here is an example of a negative movie review, associated with a rating of 2: \\*[ <...> ]\\*\n",
    "\n",
    "Now, consider the following review. Based on its content, is the sentiment of the review positive or negative? Answer with a single word. \\*[ <...> ]\\*\n",
    "\n",
    "Instructions:\n",
    "- \\*\\*Positive\\*\\* reviews typically highlight enjoyment, satisfaction, or praise for aspects of the film (e.g., acting, storyline, direction).\n",
    "- \\*\\*Negative\\*\\* reviews tend to criticize the film for its shortcomings or failures (e.g., poor pacing, bad acting, or unsatisfying plot).\n",
    "- Focus on the general tone of the review as a whole, not isolated statements or minor contradictions.\n",
    "\n",
    "Example prompts are limited to 150 words. The order of positive and negative examples is random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19af726-c4e9-4c11-8bab-8312c9225542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review length\n",
    "train['nb_words'] = train['sentiment'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b57f8bf-5ecf-4fe1-b58e-a7697d239676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompts = []\n",
    "\n",
    "# Iterate over the test dataset\n",
    "for _, test_row in tqdm(test.iterrows(),\n",
    "                        total=test.shape[0],\n",
    "                        desc = \"Generating Prompts\",\n",
    "                        unit = \"prompts\"):\n",
    "    \n",
    "    # Select a random positive and negative sentiment review from train\n",
    "    positive_row = random.choice(\n",
    "        train[(train['sentiment'] == 'positive') & (train['nb_words'] < 150)].to_dict('records'))\n",
    "    negative_row = random.choice(\n",
    "        train[(train['sentiment'] == 'negative') & (train['nb_words'] < 150)].to_dict('records'))\n",
    "\n",
    "    # Randomize the order of positive and negative examples\n",
    "    examples = random.sample(\n",
    "        [(f\"Here is an example of a positive movie review, associated with a rating of {positive_row['rating']}:\\n*[{positive_row['text']}]*\",\n",
    "          f\"Here is an example of a negative movie review, associated with a rating of {negative_row['rating']}:\\n*[{negative_row['text']}]*\"),\n",
    "         (f\"Here is an example of a negative movie review, associated with a rating of {negative_row['rating']}:\\n*[{negative_row['text']}]*\",\n",
    "          f\"Here is an example of a positive movie review, associated with a rating of {positive_row['rating']}:\\n*[{positive_row['text']}]*\")\n",
    "        ], 1)[0]\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "    {examples[0]}\n",
    "\n",
    "    {examples[1]}\n",
    "\n",
    "    Now, consider the following review. Based on its content, is the sentiment of the review positive or negative? Answer with a single word.\n",
    "    *[{test_row['text']}]*\n",
    "\n",
    "    Instructions:\n",
    "    - **Positive** reviews typically highlight enjoyment, satisfaction, or praise for aspects of the film (e.g., acting, storyline, direction).\n",
    "    - **Negative** reviews tend to criticize the film for its shortcomings or failures (e.g., poor pacing, bad acting, or unsatisfying plot).\n",
    "    - Focus on the general tone of the review as a whole, not isolated statements or minor contradictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts.append({\n",
    "        'review_id': test_row['review_id'],\n",
    "        'prompt': prompt\n",
    "    })\n",
    "    \n",
    "prompts = pd.DataFrame(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e8eb6-523a-4f32-a78b-07e871d10c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = pd.DataFrame(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22b83c-5793-4f6b-b934-b171634a85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '\\n' characters and strip leading/trailing spaces from each prompt\n",
    "def clean_prompt(prompt):\n",
    "    if isinstance(prompt, str):\n",
    "        cleaned_prompt = prompt.replace('\\n', ' ').strip()\n",
    "        return cleaned_prompt\n",
    "    return prompt  # In case it's not a string\n",
    "\n",
    "prompts['prompt'] = prompts['prompt'].apply(clean_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828cb26-f20f-4948-8b55-f09e671a5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "prompts.to_csv(\"data/processed/GTP_prompts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca05f8d-49e3-4bed-8d6c-cf78f076a42d",
   "metadata": {},
   "source": [
    "# Estimate cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f520e-bfe4-4477-9c03-2578be076495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize with tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "prompts['tokens'] = prompts['prompt'].apply(lambda x: len(enc.encode(x)))\n",
    "total_tokens = prompts['tokens'].sum()\n",
    "print(f\"Total number of tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bf8e4-1eff-44ec-a5dc-2f511ce94c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(prompts['tokens'], bins=30, color='cornflowerblue', edgecolor='midnightblue')\n",
    "plt.title('Distribution of Token Counts')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f767e-ac19-4a6a-89a3-778cb141bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o = (total_tokens * 2.5 + len(prompts) * 1.25) * 1e-6\n",
    "gpt_35_turbo = (total_tokens * 0.5 + len(prompts) * 1.5) * 1e-6\n",
    "gpt_4o_mini = (total_tokens * 0.15 + len(prompts) * 0.075) * 1e-6\n",
    "\n",
    "print(f\"Cost with gpt-4o: {gpt_4o:.2f} $\")\n",
    "print(f\"Cost with gpt-3.5-turbo: {gpt_35_turbo:.2f} $\")\n",
    "print(f\"Cost with gpt-4o-mini: {gpt_4o_mini:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf350c-c4b7-4910-8518-c3d2b4dddb42",
   "metadata": {},
   "source": [
    "# Send prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98f336-c083-4f49-a71a-616dd8f58df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the API\n",
    "client = OpenAI()\n",
    "\n",
    "def get_openai_answer(prompt):    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model = \"gpt-3.5-turbo\",\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return \"request worked\", completion\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"request failed\", str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2b887-7315-42dd-a54d-13d080400c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume API calls\n",
    "if os.path.exists(\"output/GPT_in_progress.csv\"):\n",
    "    responses = pd.read_csv(\"output/GPT_in_progress.csv\")\n",
    "    calls_left = len(responses[(responses['GPT'].isna()) | (responses['failed_calls'] > 0)])\n",
    "    print(f\"Resuming API calls - {calls_left} left\")\n",
    "\n",
    "elif os.path.exists(\"data/processed/GTP_prompts.csv\"):\n",
    "    responses = pd.read_csv(\"data/processed/GTP_prompts.csv\")\n",
    "    responses['GPT'] = pd.NA\n",
    "    responses['failed_calls'] = None\n",
    "    print(\"Starting API calls\")\n",
    "\n",
    "else:\n",
    "    print(\"Warning: regenerate prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090051d-4e0a-4227-8ef3-cf7838fa79c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows_to_process = responses[(responses['GPT'].isna()) | (responses['failed_calls'] > 0)]\n",
    "\n",
    "time0 = time.time()\n",
    "total_fails = 0\n",
    "\n",
    "if len(rows_to_process) > 0:\n",
    "    for i, row in tqdm(rows_to_process.iterrows(), total=len(rows_to_process), desc=\"Processing reviews\"):\n",
    "        review_id = row['review_id']\n",
    "        prompt = row['prompt']\n",
    "        status, response = get_openai_answer(prompt)\n",
    "\n",
    "        if status == \"request failed\":\n",
    "            responses.loc[responses['review_id'] == review_id, 'failed_calls'] = (\n",
    "                responses.loc[responses['review_id'] == review_id, 'failed_calls'] + 1\n",
    "            )\n",
    "            print(f\"Error for review #{review_id}:\\n{response}\")\n",
    "            \n",
    "            total_fails += 1\n",
    "            if total_fails >3:\n",
    "                print(\"Stopping after 3 failures...\")\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            responses.loc[responses['review_id'] == review_id, 'GPT'] = response.choices[0].message.content\n",
    "        \n",
    "        if (i + 1) % 25 == 0:\n",
    "            responses.to_csv('output/GPT_in_progress.csv', index=False)\n",
    "\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "time1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec0898-c150-4304-8307-7ea94bd24632",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"API call duration: {time1-time0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e36dea-16ac-48ee-a502-d5f0635a5f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses['GPT'] = responses['GPT'].str.lower()\n",
    "results = pd.merge(test, responses, on = \"review_id\")\n",
    "results.to_csv(\"output/GTP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf3df6-6381-46ce-bf1d-2658c23bb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(results['sentiment'], results['GPT'])\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550726c3-1b43-4a56-85a5-b8d0901e8fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
