{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68687f9f-dfc5-45ba-9bbc-9758e4b6e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39025866-c7e2-42e0-a78e-026e977f62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/processed/train.csv\")\n",
    "test = pd.read_csv(\"data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6675565-de69-4dbb-a287-d4ea82d99df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating features and labels\n",
    "# convert sentiment to binary?\n",
    "X_train = train['text']\n",
    "y_train = train['sentiment']\n",
    "\n",
    "X_test = test['text']\n",
    "y_test = test['sentiment'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d26860-0f60-4254-927c-12e990a0d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = train[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72eb1bf1-0232-496e-ab14-c6d02477f0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/39/44r9ct3n4hj3sdf26gj5wgp40000gn/T/ipykernel_8601/3227311252.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['sentiment'] = df1['sentiment'].map({'positive': 1, 'negative': 0})\n"
     ]
    }
   ],
   "source": [
    "df1['sentiment'] = df1['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02450e4b-b597-4193-a32f-8d7b4057baa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mael/.pyenv/versions/3.12.9/envs/jupyter/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "\n",
    "MAX_LEN=512\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.comment_text = dataframe[\"text\"].tolist()\n",
    "        self.targets = dataframe[\"sentiment\"].tolist()\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            #'targets': torch.tensor(targets, dtype=torch.float)\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long).to(device)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c732d27-5e50-4b17-ab5c-a4fe42a9104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5c9f97-7154-4f03-affd-7b2884799189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.2, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        config = RobertaConfig()\n",
    "        config.max_position_embeddings = 512\n",
    "        self.l1 = transformers.RobertaModel(config).from_pretrained(\"roberta-base\")\n",
    "        self.l2 = torch.nn.Dropout(0.2)\n",
    "        self.l3 = torch.nn.Linear(768,1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        output_1=self.l1(ids,token_type_ids=token_type_ids)\n",
    "        #print(out1.shape)\n",
    "        output_2 = self.l2(output_1[1])\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb42b382-58c7-4325-be42-a3d282295f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    loss=torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d06a9e43-39f9-4cfd-a637-15dac5d37bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE=1e-5\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b036d5f-5123-4239-b670-6e2e218e3149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (25000, 2)\n",
      "TRAIN Dataset: (20000, 2)\n",
      "TEST Dataset: (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset=df1.sample(frac=train_size,random_state=42)\n",
    "test_dataset=df1.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df1.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b066427b-9708-44d1-b67a-7a13ddfd6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "TRAIN_BATCH_SIZE=5\n",
    "VALID_BATCH_SIZE=1\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "train_dataloader = DataLoader(training_set, **train_params)#**passing multile parameters by dic\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96da8e81-9e58-4635-bf66-979c34679324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    average_loss=0\n",
    "    for i,data in enumerate(train_dataloader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "        #print('targets.shape',targets.shape)\n",
    "        #outputs = model(ids, mask, token_type_ids)\n",
    "        outputs = model(ids.squeeze(), mask.squeeze(), token_type_ids.squeeze())\n",
    "        #print(outputs)\n",
    "        optimizer.zero_grad()\n",
    "        # print ()\n",
    "        loss = loss_fn(outputs.squeeze(), targets)\n",
    "        average_loss+=loss\n",
    "        if i%50==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {average_loss/(i+1)}')\n",
    "        loss.backward()# caculate the derivates\n",
    "        optimizer.step()#upadte weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f57cc-a40d-4ab3-ba4e-1a9d5b35e57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mael/.pyenv/versions/3.12.9/envs/jupyter/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2690: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/var/folders/39/44r9ct3n4hj3sdf26gj5wgp40000gn/T/ipykernel_8601/1511428772.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'ids': torch.tensor(ids, dtype=torch.long),\n",
      "/var/folders/39/44r9ct3n4hj3sdf26gj5wgp40000gn/T/ipykernel_8601/1511428772.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(mask, dtype=torch.long),\n",
      "/var/folders/39/44r9ct3n4hj3sdf26gj5wgp40000gn/T/ipykernel_8601/1511428772.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.7091418504714966\n",
      "Epoch: 0, Loss:  0.6928136348724365\n",
      "Epoch: 0, Loss:  0.693176805973053\n",
      "Epoch: 0, Loss:  0.6941467523574829\n",
      "Epoch: 0, Loss:  0.6962456107139587\n",
      "Epoch: 0, Loss:  0.6950148940086365\n",
      "Epoch: 0, Loss:  0.6652579307556152\n",
      "Epoch: 0, Loss:  0.6196178793907166\n",
      "Epoch: 0, Loss:  0.5738733410835266\n",
      "Epoch: 0, Loss:  0.5373698472976685\n",
      "Epoch: 0, Loss:  0.510400116443634\n",
      "Epoch: 0, Loss:  0.490872859954834\n",
      "Epoch: 0, Loss:  0.47778740525245667\n",
      "Epoch: 0, Loss:  0.462095707654953\n",
      "Epoch: 0, Loss:  0.4427928924560547\n",
      "Epoch: 0, Loss:  0.43010213971138\n",
      "Epoch: 0, Loss:  0.41652747988700867\n",
      "Epoch: 0, Loss:  0.4058190584182739\n",
      "Epoch: 0, Loss:  0.3942759931087494\n",
      "Epoch: 0, Loss:  0.3821165859699249\n",
      "Epoch: 0, Loss:  0.3724372982978821\n",
      "Epoch: 0, Loss:  0.36300548911094666\n",
      "Epoch: 0, Loss:  0.35685163736343384\n",
      "Epoch: 0, Loss:  0.35006871819496155\n",
      "Epoch: 0, Loss:  0.3438358008861542\n",
      "Epoch: 0, Loss:  0.3391912579536438\n",
      "Epoch: 0, Loss:  0.3332423269748688\n",
      "Epoch: 0, Loss:  0.3273794651031494\n",
      "Epoch: 0, Loss:  0.3233907222747803\n",
      "Epoch: 0, Loss:  0.3179699778556824\n",
      "Epoch: 0, Loss:  0.3137702941894531\n",
      "Epoch: 0, Loss:  0.30999431014060974\n",
      "Epoch: 0, Loss:  0.3102218508720398\n",
      "Epoch: 0, Loss:  0.3074898421764374\n",
      "Epoch: 0, Loss:  0.3037970960140228\n",
      "Epoch: 0, Loss:  0.30010128021240234\n",
      "Epoch: 0, Loss:  0.296265572309494\n",
      "Epoch: 0, Loss:  0.2934834361076355\n",
      "Epoch: 0, Loss:  0.29019635915756226\n",
      "Epoch: 0, Loss:  0.28786078095436096\n",
      "Epoch: 0, Loss:  0.2868387997150421\n",
      "Epoch: 0, Loss:  0.28491607308387756\n",
      "Epoch: 0, Loss:  0.28266245126724243\n",
      "Epoch: 0, Loss:  0.28127920627593994\n",
      "Epoch: 0, Loss:  0.2804996371269226\n",
      "Epoch: 0, Loss:  0.27913734316825867\n",
      "Epoch: 0, Loss:  0.27676182985305786\n",
      "Epoch: 0, Loss:  0.2761472761631012\n",
      "Epoch: 0, Loss:  0.2752586305141449\n",
      "Epoch: 0, Loss:  0.27393704652786255\n",
      "Epoch: 0, Loss:  0.2722346782684326\n",
      "Epoch: 0, Loss:  0.2718088924884796\n"
     ]
    }
   ],
   "source": [
    "# As after each epochs i save the mdole,so previous epochs results are not listed below, the total number of epoch thats model run is 25.\n",
    "model_path = \"model/\"\n",
    "\n",
    "EPOCHS=5\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b9006-3cc9-402e-b987-557d4709aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    #softm = torch.nn.sigmoid(dim=1)\n",
    "    sigmoid_v=torch.nn.Sigmoid()\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            #print(ids.shape, mask.shape,token_type_ids.shape,targets.shape)\n",
    "            ids=ids.squeeze()\n",
    "            mask=mask.squeeze()\n",
    "            token_type_ids=token_type_ids.squeeze()\n",
    "            outputs = model(ids.unsqueeze(0), mask.unsqueeze(0), token_type_ids.unsqueeze(0))\n",
    "            #print(targets.shape)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(sigmoid_v(outputs))\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4873d3c6-b09a-41be-87a4-8be899ac1a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "for epoch in range(1):\n",
    "    outputs, targets = validation(epoch)\n",
    "    #targets=targets\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    precision = precision_score(targets, outputs)\n",
    "    recall = recall_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"Precision = {precision}\")\n",
    "    print(f\"Recall = {recall}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "    print(confusion_matrix(targets, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550ae6a-2675-4196-ac5e-1f0699f7475c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
